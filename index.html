<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
p, ul, ol {
  text-align: justify !important;
}
.responsive-media {
    width: 100%;
    margin: 0;
}
@media (min-width: 768px) {
    .responsive-media {
        width: 95%;
    }
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}

.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}

.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

pre {
  display: flex;
  justify-content: center;
  overflow-x: auto;
  max-width: 100%;
  padding: 0 1rem;
  box-sizing: border-box;
}

pre code {
  display: inline-block;
  text-align: left;
  padding: 10px;
  border-radius: 5px;
  background: #f5f5f5;
  margin: 0;
  max-width: 100%;
  box-sizing: border-box;
}

.katex-display {
  overflow-x: auto;
  overflow-y: hidden;
  white-space: nowrap;
}

pre.citation {
  justify-content: flex-start;
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}

.btn-group button {
  background-color: orange;
  border: 1px solid #FF6C00;
  color: white;
  padding: 5px 12px;
  cursor: pointer;
  float: center;
}

button {
  margin: 10px;
  padding: 10px 15px;
  font-size: 16px;
  cursor: pointer;
}

.output-container {
  margin-top: 0px;
  font-size: 20px;
  font-weight: bold;
}

#traceChartsContainer {
  display: grid;
  grid-template-columns: repeat(8, 1fr);
  gap: 10px;
}
    
#traceChartsContainer canvas {
  width: 100%;
  height: auto;
}
#loadingSpinner {
    position: absolute;
    z-index: 3;
    display: block;
}

.chart-container {
  width: 90vw;
  max-width: 400px;
  height: auto;
  margin: 0 auto;
}

.spinner {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    height: 100px;
    width: 100px;
    z-index: 5;
    
    -webkit-animation: rotation .6s infinite linear;
    animation: rotation .6s infinite linear;
    
    border-left: 6px solid rgba(0, 174, 239, .15);
    border-right: 6px solid rgba(0, 174, 239, .15);
    border-bottom: 6px solid rgba(0, 174, 239, .15);
    border-top: 6px solid rgba(0, 174, 239, .8);
    border-radius: 50%;
}

@keyframes rotation {
    from { transform: translate(-50%, -50%) rotate(0deg); }
    to { transform: translate(-50%, -50%) rotate(360deg); }
}

#canvasContainer {
  position: relative;
  width: 280px;
  height: 280px;
  margin: 0 auto;
}

#drawCanvas, #attentionCanvas {
    position: absolute;
    left: 0;
    top: 0;
    width: 100%;
    height: 100%;
    touch-action: none;
}

#drawCanvas {
    background-color: black;
    cursor: crosshair;
    z-index: 1;
}

#attentionCanvas {
    z-index: 2;
    pointer-events: none;
}

@media (max-width: 768px) {
    #canvasContainer {
        width: 280px;
        height: 280px;
    }
    #drawCanvas, #attentionCanvas {
        width: 280px;
        height: 280px;
    }
    .chart-container {
        width: 400px;
    }
}

@media (max-width: 480px) {
    #canvasContainer {
        width: 280px;
        height: 280px;
    }
    .chart-container {
        width: 400px;
    }
}

/* Add a background color on hover */
.btn-group button:hover {
  background-color: #FF6C00;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <link href="https://vjs.zencdn.net/8.11.8/video-js.css" rel="stylesheet" />
  <link rel="stylesheet" href="css/style_mazes.css">
  <link rel="stylesheet" href="css/style_videos.css">
  <link rel="stylesheet" href="css/style_experiments.css">
  <link rel="stylesheet" href="css/snippet.css">
  

  <meta name="theme-color" content="#ffffff" />


  <!-- SEO -->

  <!-- TO DO: put in the meta cards for social media sharing pages -->
  <!-- SEO -->
  <meta property="og:title" content="Continuous Thought Machines" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Introducing Continuous Thought Machines: a new kind of neural network model that unfolds and uses neural dynamics as a powerful representation for thought." />
  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Thought Machines" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="Continuous Thought Machines" />
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@latest/dist/ort.min.js"></script>
</head>
<link rel="stylesheet" href="css/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>
<script src="https://vjs.zencdn.net/8.11.8/video.min.js"></script>
<script src="results_video.js"></script>

<script type="text/front-matter">
  title: "Continuous Thought Machines"
  description: ""
</script>
<body>
  <div id="no_javasript_warning">
    <h3>This page requires Javascript. Please enable it to view the website.</code></h3>
  </div>
  <script>
    document.getElementById("no_javasript_warning").style.display = "none";
  </script>



<dt-article id="dtbody">


<dt-byline class="l-page transparent"></dt-byline>
<h1>Continuous Thought Machines</h1>
<div style="text-align: center;">
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">
  tl;dr
</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
  Neurons in brains use timing and synchronization in the way that they compute. This property seems essential for the flexibility and adaptability of biological intelligence. Modern AI systems discard this fundamental property in favor of efficiency and simplicity. We found a way of bridging the gap between the existing powerful implementations and scalability of modern AI, and the biological plausibility paradigm where <b>neuron timing matters</b>. The results have been surprising and encouraging.
</br>
</figcaption>
</div>
<p></p>

</dt-byline>
<div class="main-container" id="maze-demo-container">
    <h2>Interactive demonstration</h2>
    <div class="content-wrapper">
        <div class="maze-column" id='maze-demo'>
            <!-- <div id="maze-load-overlay" class="maze-load-overlay loading-overlay-inactive">
                <div class="overlay-text">Click/Touch to load maze demo</div>
            </div> -->
            <div id="status">Initializing...</div>
            <div id="loadingIndicator" class="loading-indicator">
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
                <div class="loading-bar"></div>
            </div>
            <canvas id="mazeCanvas" width="39" height="39"></canvas>
            <div id="attentionHeadsContainer" class="attention-grid">
                </div>
            <div id="controls">
                <div class="control-group button-group">
                    <button id="solveButton" disabled>Run</button>
                    <button id="teleportButton" disabled>Teleport</button>
                    <button id="loadNewMazeButton" disabled>New</button>
                    <!-- <button id="skipAnimationButton" disabled>Skip</button> -->
                    <button id="toggleModeButton">Move:🟥</button>
                </div>
                <div class="control-group checkbox-group">
                    <div>
                        <div>
                            <input type="checkbox" id="validOnlyCheckbox" checked>
                            <label for="validOnlyCheckbox">Valid Path Only</label>
                        </div>
                        <div>
                            <input type="checkbox" id="autoSolveCheckbox" checked>
                            <label for="autoSolveCheckbox">Auto-solve</label>
                        </div>
                    </div>
                    <div style="margin-top: 1px;">
                        <div>
                            <input type="checkbox" id="showPathCheckbox" checked>
                            <label for="showPathCheckbox">Show Path</label>
                        </div>
                        <div>
                            <input type="checkbox" id="showOverlayCheckbox" checked>
                            <label for="showOverlayCheckbox">Show Attention Overlay</label>
                        </div>
                    </div>
                </div>
                <div class="control-group slider-group">
                    <label for="fpsSlider">Animation FPS:</label>
                    <input type="range" id="fpsSlider" name="fps" min="1" max="120" value="60" step="1">
                    <span id="fpsValueDisplay">60</span>
                </div>
            </div> 
            <div id="canvasHint" class="canvas-hint">
                Click to move Start/ End (toggle with 'move')
            </div>
            </div> 
                <div class="sidebar-column">
                <img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/brain.png" style="display: block; margin: auto; width: 100%;"/>
                <p id="sidebarDescription">
                    This is the Continuous Thought Machine (CTM): a new type of neural network that uses the synchronization of neural activity over time as its representation for taking actions in a world. 
                </p>
                <p id="sidebarDescription">
                    This maze solving demo runs a <b>real CTM in your browser</b>: it tries to find a path (up to 150 steps) from the red pixel to the green pixel. 
                </p>
                <p id="sidebarDescription">
                    <b>Hit the <span class="text-highlight-blue">teleport</span> button to move the start location to the last step the CTM predicts, and have fun teleporting to solve longer paths! </b>
                </p>
                <p id="sidebarDescription">
                    You can also hit the <span class="text-highlight-blue">run</span> button to watch it solve again. You can move the <span class="text-highlight-red">start</span> or <span class="text-highlight-green">end</span> positions by clicking on different maze locations. Toggle the <span class="text-highlight-red">m</span><span class="text-highlight-green">o</span><span class="text-highlight-red">v</span><span class="text-highlight-green">e</span> button to switch between moving start or end positions.
                </p>
                <p id="sidebarDescription">
                    As it pays attention to the maze (shown below the maze and overlayed brightly) it unfolds <b>Neural Dynamics</b>: the complex, time-dependent patterns of neural activity. The way that neurons synchronize is how the CTM interacts with the world. Watch how it builds a route as it thinks. Have fun trying to solve these mazes quickly with the CTM!
                </p>
                <p id="sidebarDescription">
                    Note that this is a much smaller model than what we trained for <a href="#maze-results">the full results</a>, and it might not always perform as consistently well as what those results show.
                </p>
                </div> 
            </div> 
        </div>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="./maze_solver.js" type="module"></script>
</div>
<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']], // Define $...$ and \(...\) as inline math
        displayMath: [['$$', '$$'], ['\\[', '\\]']], // Define $$...$$ and \[...\] as display math
        processEscapes: true // Allows \$ to be treated as a literal dollar sign
      },
      svg: { // Use SVG output (often looks best)
        fontCache: 'global'
      }
      // Or use CHTML output:
      // chtml: {
      //   fontURL: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2'
      // }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script>
<h2>Introduction</h2>
<p>Neural networks (NNs) were originally inspired by biological brains, yet they remain significantly distinct from their biological counterparts. Brains demonstrate complex neural dynamics that evolve over time, but modern NNs intentionally abstract away such temporal dynamics in order to facilitate large-scale deep learning. For instance, the activation functions of standard NNs can be seen as an intentional abstraction of a neuron's firing rate, replacing the temporal dynamics of biological processes with a single, static value. Such simplifications, though enabling significant advancements in large-scale machine learning <dt-cite key="lecun2015deep,goodfellow2016deep,wei2022emergent"></dt-cite>, have resulted in a departure from the fundamental principles that govern biological neural computation.</p>
<p>Over hundreds of millions of years, evolution has endowed biological brains with rich neural dynamics, including spike-timing-dependent plasticity (STDP) <dt-cite key="caporale2008spike"></dt-cite> and neuronal oscillations. Emulating these mechanisms, particularly the temporal coding inherent in spike timing and synchrony, presents a significant challenge. Consequently, modern neural networks do not rely on temporal dynamics to perform compute, but rather prioritize simplicity and computational efficiency. This abstraction, while boosting performance on specific tasks, contributes to a recognized gap between the flexible, general nature of human cognition and current AI capabilities, suggesting fundamental components, potentially related to temporal processing, are missing from our current models <dt-cite key="lake2017building,marcus2018deep,chollet2019measure"></dt-cite>.</p>
<div class="snippet-box">
  <h4 class="snippet-title">Why do this research?</h4>
  <div class="snippet-content">
    <p>Indeed, the notably high performance of modern AI across many fields suggests the emulation of neural dynamics is unwarranted. However, the gap between the highly flexible and general nature of human cognition and the current state of modern AI suggests missing components in our current models.</p>
  </div>
</div>
<p>For these reasons, we argue that time should be a central component of artificial intelligence in order for it to eventually achieve levels of competency that rival or surpass human brains <dt-cite key="cariani2022time,maass2001relevance"></dt-cite>. Therefore, in this work, we address the strong limitation imposed by overlooking neural activity as a central aspect of intelligence. We introduce the <strong>Continuous Thought Machine (CTM)</strong>, a novel neural network architecture designed to explicitly incorporate neural timing as a foundational element. Our contributions are as follows:</p>
<ul>
<li>We introduce a <strong>decoupled internal dimension</strong>, a novel approach to modeling the temporal evolution of neural activity. We view this dimension as that over which thought can unfold in an artificial neural system, hence the choice of nomenclature.</li>
<li>We provide a mid-level abstraction for neurons, which we call <strong>neuron-level models</strong> (NLMs), where every neuron has its own internal weights that process a history of incoming signals (i.e., pre-activations) to activate (as opposed to a static ReLU, for example).</li>
<li>We use <strong>neural synchronization</strong> directly as the latent representation with which the CTM observes (e.g., through an attention query) and predicts (e.g., via a projection to logits). This biologically-inspired design choice puts forward neural activity as the crucial element for any manifestation of intelligence the CTM might demonstrate.</li>
</ul>
<h4>Reasoning models and recurrence</h4>
<p>The frontier of artificial intelligence faces a critical juncture: moving beyond simple input-output mappings towards genuine reasoning capabilities. While scaling existing models has yielded remarkable advancements, the associated computational cost and data demands are unsustainable and raise questions about the long-term viability of this approach. For sequential data, longstanding recurrent architectures <dt-cite key="hochreiter1997long,dey2017gate,medsker1999recurrent"></dt-cite> have largely been superseded by transformer-based approaches <dt-cite key="vaswani2017attention"></dt-cite>. Nevertheless, recurrence is re-emerging as a natural avenue for extending model complexity. Recurrence is promising because it enables iterative processing and the accumulation of information over time. Modern text generation models (sometimes referred to as 'reasoning models') use intermediate generations as a form of recurrence that enables additional compute during test-time. Recently, other works have demonstrated the benefits of the recurrent application of latent layers <dt-cite key="jaegle2021perceiver,geiping2025scaling,yang2023looped"></dt-cite>. While such methods bring us closer to the recurrent structure of biological brains, a fundamental gap nevertheless remains. <strong>We posit that recurrence, while essential, is merely one piece of the puzzle</strong>. The temporal dynamics unlocked by recurrence -- the precise timing and interplay of neural activity -- are equally crucial. The CTM differs from existing approaches in three ways: (1) the decoupled internal dimension enables sequential thought on any conceivable data modality; (2) private neuron-level models enables the consideration of precise neural timing; and (3) neural synchronization used directly as a representation for solving tasks.</p>
<hr>
<h2>Method</h2>
<p><video src="assets/mp4/blog_arch_video.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video" id="scroll-play-video"></video></p>
<script src="./scroll_play.js"></script>
<div style="text-align: center;">
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;">
<b>Fig 1.</b> The Continuous Thought Machine: <span style="color:#000000">a single step in its internal recurrent process.</span>
</figcaption>
<figcaption style="text-align: left; padding-top: 0;">
The CTM unfolds neural activity internally as it thinks about data. At each step (one of which demonstrated above) a truncated history of 'pre activations' are collected and used for the <b>Neuron Level Models</b> (NLMs). The history of 'post activations' produced by all NLMs over time are kept and used to compute neuron-to-neuron synchronization over time. This result is a <b>Synchronization Representation</b>: a new, parameter-efficient, and <i>evidently powerful</i> representation that the CTM uses to observe (via attention) and predict.
</br>
</figcaption>
</div>
<p>The Continuous Thought Machine (CTM) is a neural network architecture that enables a novel approach to thinking about data. It departs from conventional feed-forward models by explicitly incorporating the concept of <strong>Neural Dynamics</strong> as the central component to its functionality. The video above gives a pictorial overview of the internal workings of the CTM. We will provide links to relevant parts of the repository as we explain the model below.</p>
<div class="image-table-layout">
    <figure class="layout-item image-item">
        <img src="assets/png/architecture.jpeg" alt="CTM architecture">
        <figcaption style="text-align:justify;">
        <span class="caption-highlight"><b>Fig 2.</b> CTM architecture</span>: The <span class="circled-number">1</span> synapse model (weights depicted as blue lines) models the cross-neuron interactions to produce pre-activations. For each neuron, a <span class="circled-number">2</span> history of pre-activations is kept, the most recent of which are used by the <span class="circled-number">3</span> neuron-level models (weights depicted as red lines) to produce <span class="circled-number">4</span> post-activations. A <span class="circled-number">5</span> history of post-activations is also kept and used to <span class="circled-number">6</span> compute a synchronization matrix. Neuron pairs are <span class="circled-number">7</span> selected from the synchronization matrix, yielding the <span class="circled-number">8</span> latent representations with which the CTM <span class="circled-number">9</span> produces outputs and modulates data through cross-attention. Modulated data (e.g., attention outputs) are <span class="circled-number">10</span> concatenated with post-activations for the next internal tick.
        </figcaption>
    </figure>
    <figure class="layout-item table-item">
        <table class="legend-table">
            <thead>
                <tr>
                    <th scope="col">Variable</th>
                    <th scope="col">Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>$\mathbf{z}^t$</td> 
                    <td>Post-activations at internal tick $t$, after neuron-level models have been used.</td> 
                </tr>
                <tr>
                    <td>$\theta_{\text{syn}}$</td>
                    <td>Recurrent (synapse) model weights; U-NET-like architecture that connects neurons at a given internal tick, $t$.</td>
                </tr>
                <tr>
                    <td>$\mathbf{a}^t$</td>
                    <td>Pre-activations at internal tick $t$.</td>
                </tr>
                <tr>
                    <td>$\mathbf{A}^t$</td>
                    <td>History of <i>most recent</i> pre-activations, designed as a FIFO list so that they are always length $M$; inputs to neuron-level models.</td>
                </tr>
                <tr>
                    <td>$\theta_{\text{d}}$</td>
                    <td>Weights of a <i>single neuron-level model</i>, $d$ of $D$; MLP architecture, unique weights per neuron.</td>
                </tr>
                <tr>
                    <td>$\mathbf{Z}^t$</td>
                    <td>History of <i>all</i> post-activations up to this internal tick, variable length; used as input for synchronization dot products.</td>
                </tr>
                <tr>
                    <td>$\mathbf{S}^t$</td>
                    <td>Synchronization matrix at internal tick $t$. In practice we use far fewer neurons than $D$ for separate $\mathbf{S}^t_{\text{out}}$ and $\mathbf{S}^t_{\text{action}}$ synchronization representations.</td>
                </tr>
                <tr>
                    <td>$\mathbf{W}_{\text{out}}$, $\mathbf{W}_{\text{in}}$</td>
                    <td>Linear weight matrices that project from $\mathbf{S}^t_{\text{out}}$ and $\mathbf{S}^t_{\text{action}}$ to attention queries and predictions, respectively.</td>
                </tr>
                <tr>
                    <td>$\mathbf{o}^t$</td> 
                    <td>Cross attention output.</td> 
                </tr>
                </tbody>
        </table>
    </figure>
</div>
<p><strong>The CTM consists of three main ideas</strong>:</p>
<ol>
<li>The use of <a href='#internal-ticks'>internal recurrence</a>, enabling a dimension over which a concept analogous to <strong>thought</strong> can occur. The entire process visualised in the video above is a single tick; the <a href='#maze-demo'>interactive maze demo</a> at the top of the page uses 75 ticks. This recurrence is completely decoupled from any data dimensions.</li>
<li><a href='#neuron-level-models'>Neuron-level models</a>, that compute post-activations by applying private (i.e., on a per-neuron basis) MLP models to a <em>history of incoming pre-activations</em>.</li>
<li><a href='#synchronization-representation'>Synchronization as a representation</a>, where the neural activity over time is tracked and used to compute how pairs of neurons synchronize with one another over time. This measure of synchronization is the representation with which the CTM takes action and makes predictions.</li>
</ol>
<div class="snippet-box">
  <h4 class="snippet-title">But what about data?</h4>
  <div class="snippet-content">
    <p>While data is undoubtedly crucial for any modeling, the CTM is designed around the idea of internal recurrence and synchronization, where the role of data is somewhat secondary to the internal process itself.</p>
    <p><a href="#from-data">Input data is attended to</a> and ingested at each internal tick based on the current sychronisation, and similarly for predictions.</p>
  </div>
</div>
<td id='neural-dynamics', style="width: 100%;border: 0px solid transparent;"><video src="assets/mp4/topvideo.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video></td>
<div style="text-align: center;">
<figcaption style="text-align: left; color:#FF6C00; padding-top: 0;"><b>Fig 3.</b> Neural Dynamics when thinking about ImageNet: <span style="color:#000000">Each subplot is the activity of a single neuron over time. It is the synchronization between these that forms the representation used by the CTM.</span>
</figcaption>
</div>
<h3 id="internal-ticks">Internal ticks: the 'thought' dimension</h3>
<p>We start by introducing the continuous internal dimension: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">t \in  \{ 1, \ldots ,T \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mrel">∈</span><span class="mopen">{</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.13889em;">T</span><span class="mclose">}</span></span></span></span>. Unlike conventional sequential models -- such as RNNs or Transformers -- that process inputs step-by-step according to the sequence inherent in the data (e.g., words in a sentence or frames in a video), the CTM operates along a self-generated timeline of internal <strong>thought steps</strong>. This internal unfolding allows the model to iteratively build and refine its representations, even when processing static or non-sequential data such as images or mazes. To conform with existing nomenclature used in related works <dt-cite key="kirsch2021meta,pedersen2024structurally,kirsch2022introducing,schwarzschild2021can"></dt-cite>, we refer to these thought steps as 'internal ticks' from here on.</p>
<div class="snippet-box">
  <h4 class="snippet-title">A dimension over which thought can unfold.</h4>
  <div class="snippet-content">
    <p>The CTM's internal dimension is that over which the dynamics of neural activity can unfold. We believe that such dynamics are likely a cornerstone of intelligent thought.</p>
  </div>
</div>
<h3 id='synapses'>Recurrent weights: synapses</h3>
<p>A recurrent multi-layer perceptron (MLP structured in a U-NET fashion <dt-cite key="ronneberger2015u"></dt-cite>) acts as a synapse model for the CTM. At any internal tick <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span>, the synapse model produces what we consider <strong>pre-activations</strong>:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mi>f</mi><mrow><msub><mi>θ</mi><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow></msub><mo>(</mo><mtext><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi></mtext><mo>(</mo><msup><mrow><mi mathvariant="bold">z</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup><mo>)</mo><mo>)</mo><mo>∈</mo><mtext> </mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>D</mi></msup><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bold{a}^t = f_{\theta_{\text{syn}}}(\text{concat}(\bold{z}^t, \bold{o}^t)) \in~\mathbb{R}^D,
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8913309999999999em;"></span><span class="strut bottom" style="height:1.2435509999999999em;vertical-align:-0.35222em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">a</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.14999999999999997em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="text mord scriptscriptstyle cramped"><span class="mord mathrm">s</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">c</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span><span class="mord mathrm">c</span><span class="mord mathrm">a</span><span class="mord mathrm">t</span></span><span class="mopen">(</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">z</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">o</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mrel">∈</span><span class="mord mspace"> </span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">D</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{o}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">o</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is <a href="#from-data">from input data</a>. The <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> <strong>most recent pre-activations</strong> are then collected into a pre-activation 'history':</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">A</mi></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mrow><mi>t</mi><mo>−</mo><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mrow><mi>t</mi><mo>−</mo><mi>M</mi><mo>+</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mo>⋯</mo></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mi>t</mi></msup></mrow></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><mtext> </mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>M</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{A}^t = \begin{bmatrix}
\bold{a}^{t-M+1} &amp; \bold{a}^{t-M+2} &amp; \cdots &amp; \bold{a}^t
\end{bmatrix} \in~\mathbb{R}^{D \times M}.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.891331em;"></span><span class="strut bottom" style="height:1.2669965em;vertical-align:-0.37566550000000004em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">A</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist"><span style="top:0.0156655em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">a</span></span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist"><span style="top:0.0156655em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">a</span></span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mbin">+</span><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist"><span style="top:0.0156655em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="minner">⋯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist"><span style="top:0.0156655em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">a</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">]</span></span></span><span class="mrel">∈</span><span class="mord mspace"> </span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mbin">×</span><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">.</span></span></span></span></span></p>
<h3 id='neuron-level-models'>Neuron-level models</h3>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> effectively defines the length of the <strong>history of pre-activations</strong> that each neuron level model works with. Each neuron, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>D</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">\{1, \ldots, D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">{</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">}</span></span></span></span>, is then <strong>given its own privately parameterized MLP</strong> that produces what we consider <strong>post-activations</strong>:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">z</mi></mrow><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>g</mi><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow></msub><mo>(</mo><msubsup><mrow><mi mathvariant="bold">A</mi></mrow><mi>d</mi><mi>t</mi></msubsup><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bold{z}_d^{t+1} = g_{\theta_d}(\bold{A}_d^t),
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8641079999999999em;"></span><span class="strut bottom" style="height:1.1555469999999999em;vertical-align:-0.2914389999999999em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">z</span></span><span class="vlist"><span style="top:0.2914389999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">d</span></span></span><span style="top:-0.4129999999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15122857142857138em;margin-right:0.07142857142857144em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">d</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">A</span></span><span class="vlist"><span style="top:0.247em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">d</span></span></span><span style="top:-0.4129999999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\theta_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">d</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are the unique parameters for neuron <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span></span></span></span>, and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">z</mi></mrow><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\bold{z}_d^{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.854239em;"></span><span class="strut bottom" style="height:1.1555469999999999em;vertical-align:-0.3013079999999999em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">z</span></span><span class="vlist"><span style="top:0.3013079999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">d</span></span></span><span style="top:-0.403131em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is a single unit in the vector that contains all <strong>post-activations</strong>. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">A</mi></mrow><mi>d</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{A}_d^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:1.0766639999999998em;vertical-align:-0.2831079999999999em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">A</span></span><span class="vlist"><span style="top:0.2831079999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">d</span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>-dimensional vector (time series). The full set of neuron post-activations are then concatenated with  <a href="#from-data">attention output</a> and fed recurrently into <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mrow><msub><mi>θ</mi><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">f_{\theta_{\text{syn}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:1.04666em;vertical-align:-0.35222em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.14999999999999997em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="text mord scriptscriptstyle cramped"><span class="mord mathrm">s</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to produce pre-activations for next step, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span>, in the unfolding thought process.</p>
<h3 id="synchronization-representation">Synchronization as a representation: modulating data</h3>
<p>How should the CTM interact with the outside world? Specifically, how should the CTM consume inputs and produce outputs? We introduced a timing dimension over which something akin to thought can unfold. We also want the CTM's relationship with data (its interaction, so to speak) to depend not on a <em>snapshot</em> of the state of neurons (at some <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span>), but rather on the <strong>ongoing temporal dynamics of neuron activities</strong>. By way of solution, we turn again to natural brains for inspiration and find the concept of neural synchronization <dt-cite key="uhlhaas2009neural"></dt-cite> both fitting and powerful. For synchronization we start by collecting the post-activations into a post-activation 'history':</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mn>1</mn></mrow></msup></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mo>⋯</mo></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mi>t</mi></msup></mrow></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>t</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{Z}^t = \begin{bmatrix}
\bold{z}^{1} &amp; \bold{z}^{2} &amp; \cdots &amp; \bold{z}^t
\end{bmatrix} \in \mathbb{R}^{D \times t}.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.891331em;"></span><span class="strut bottom" style="height:1.253385em;vertical-align:-0.36205399999999993em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">Z</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist"><span style="top:0.0020539999999998892em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">z</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist"><span style="top:0.0020539999999998892em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">z</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist"><span style="top:0.0020539999999998892em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="minner">⋯</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist"><span style="top:0.0020539999999998892em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">z</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">]</span></span></span><span class="mrel">∈</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mbin">×</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">.</span></span></span></span></span></p>
<p>The length of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{Z}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">Z</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is equal to the current internal tick, meaning that <strong>this dimension is not fixed</strong> and can be arbitrarily large. We define neural synchronization as the matrix yielded by the inner dot product between post-activation histories:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">S</mi></mrow><mi>t</mi></msup><mo>=</mo><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><mo>⋅</mo><mo>(</mo><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><msup><mo>)</mo><mo>⊺</mo></msup><mo>∈</mo><mtext> </mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>D</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{S}^t = \bold{Z}^t \cdot (\bold{Z}^t)^\intercal \in~\mathbb{R}^{D\times D}.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.891331em;"></span><span class="strut bottom" style="height:1.141331em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">Z</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">⋅</span><span class="mopen">(</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">Z</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord amsrm">⊺</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mord mspace"> </span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.41300000000000003em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mbin">×</span><span class="mord mathit" style="margin-right:0.02778em;">D</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">.</span></span></span></span></span></p>
<p>Since this matrix scales in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>D</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(D^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> it makes practical sense to subsample <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05724em;">j</span><span class="mclose">)</span></span></span></span> row-column pairs, which capture the synchronization between neurons <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span>. To do so we randomly select <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{action}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05724em;">j</span><span class="mclose">)</span></span></span></span> pairs from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">S</mi></mrow></mrow><annotation encoding="application/x-tex">\bold{S}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathbf">S</span></span></span></span></span>, thus collecting two <strong>synchronization representations</strong>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup><mo>∈</mo><mtext> </mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{out} \in~\mathbb{R}^{D_\text{out}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.088331em;vertical-align:-0.247em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:0.247em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mord mspace"> </span><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="text mord scriptscriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup><mo>∈</mo><mtext> </mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{action} \in~\mathbb{R}^{D_\text{action}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.105833em;vertical-align:-0.264502em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:0.264502em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mord mspace"> </span><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="text mord scriptscriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:1.040556em;vertical-align:-0.247em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:0.247em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> can then be projected to an output space as:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></mrow></msub><mo>⋅</mo><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{y}^t = \bold{W}_{\text{out}} \cdot \bold{S}^t_\text{out}.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.843556em;"></span><span class="strut bottom" style="height:1.0905559999999999em;vertical-align:-0.247em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">⋅</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:0.247em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span><span style="top:-0.4129999999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">.</span></span></span></span></span></p>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<div class="snippet-box">
  <h4 class="snippet-title">Synchronization enables a very large representation.</h4>
  <div class="snippet-content">
    <p>
      As the model width, D, grows, the synchronization representation grows with
      \(\frac{D \times (D+1)}{2}\), offering opportunities for improved expressiveness without the need for more parameters in order to project a latent space to this size.
    </p>
  </div>
</div>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<h4 id='from-data'>Modulating input data</h3>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{action}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:1.058058em;vertical-align:-0.264502em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:0.264502em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> can be used to take actions in the world (e.g., via attention as is in our setup):</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub><mo>⋅</mo><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{q}^t = \bold{W}_{\text{in}} \cdot \bold{S}^t_\text{action}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.843556em;"></span><span class="strut bottom" style="height:1.0905559999999999em;vertical-align:-0.247em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">i</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">⋅</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:0.247em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span></span></span><span style="top:-0.4129999999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\bold{W}_{\text{out}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\bold{W}_{\text{in}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">i</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are learned weight matrices that project synchronization into vectors for observation (e.g., attention queries, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{q}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.9879959999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>) or outputs (e.g., logits, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{y}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.9879959999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>). Even though there are <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>D</mi><mo>×</mo><mo>(</mo><mi>D</mi><mo>+</mo><mn>1</mn><mo>)</mo><mo>)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">(D \times (D+1))/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mbin">×</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord mathrm">/</span><span class="mord mathrm">2</span></span></span></span> unique pairings in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">S</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">S</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{action}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> can be orders of magnitude smaller than this. That said, the full synchronization matrix is a large representation that has high future potential.</p>
<p>In most of our experiments we used standard cross attention <dt-cite key="vaswani2017attention"></dt-cite>:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup><mo>=</mo><mtext><mi mathvariant="normal">A</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mo>(</mo><mi>Q</mi><mo>=</mo><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><mi>K</mi><mi>V</mi><mo>=</mo><mtext><mi mathvariant="normal">F</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi></mtext><mo>(</mo><mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi></mtext><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\bold{o}^t = \text{Attention}(Q=\bold{q}^t, KV=\text{FeatureExtractor}(\text{data}))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.843556em;"></span><span class="strut bottom" style="height:1.093556em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">o</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">A</span><span class="mord mathrm">t</span><span class="mord mathrm">t</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span><span class="mopen">(</span><span class="mord mathit">Q</span><span class="mrel">=</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf">q</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">F</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">t</span><span class="mord mathrm">u</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">E</span><span class="mord mathrm">x</span><span class="mord mathrm">t</span><span class="mord mathrm">r</span><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">t</span><span class="mord mathrm">o</span><span class="mord mathrm">r</span></span><span class="mopen">(</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">d</span><span class="mord mathrm">a</span><span class="mord mathrm">t</span><span class="mord mathrm">a</span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>where a 'FeatureExtractor' model, e.g., a ResNet <dt-cite key="he2016deep"></dt-cite>, is first used to build useful local features for the keys and values. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mrow><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{o}^{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">o</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is concatenated with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{z}^{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf">z</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> for the next cycle of recurrence.</p>
<h3 id='loss-function'>Loss function: optimizing across internal ticks</h3>
<p>The CTM produces outputs at each internal tick, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span>. A key question arises: how do we optimize the model across this internal temporal dimension?  Let <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{y}^t \in \mathbb{R}^{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∈</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> be the prediction vector (e.g., probabilities of classes) at internal tick <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span> is the number of classes.  Let <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{true}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">u</span><span class="mord mathit">e</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> be the ground truth target. We can compute a loss at each internal tick using a standard loss function, such as cross-entropy:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mi>t</mi></msup><mo>=</mo><mtext><mi mathvariant="normal">C</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">y</mi></mtext><mo>(</mo><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">    \mathcal{L}^t = \text{CrossEntropy}(\bold{y}^t, y_{true}),
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.843556em;"></span><span class="strut bottom" style="height:1.093556em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathcal">L</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">C</span><span class="mord mathrm">r</span><span class="mord mathrm">o</span><span class="mord mathrm">s</span><span class="mord mathrm">s</span><span class="mord mathrm">E</span><span class="mord mathrm">n</span><span class="mord mathrm">t</span><span class="mord mathrm">r</span><span class="mord mathrm">o</span><span class="mord mathrm">p</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span></span><span class="mopen">(</span><span class=""><span class="mord displaystyle textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">u</span><span class="mord mathit">e</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></p>
<p>and a corresponding certainty measure, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="script">C</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.05834em;">C</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. We compute certainty simply as 1 - normalised entropy. We compute <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{L}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">L</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi mathvariant="script">C</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.05834em;">C</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> for all <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">t \in \{1, \ldots, T\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mrel">∈</span><span class="mopen">{</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="minner">…</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.13889em;">T</span><span class="mclose">}</span></span></span></span>, yielding losses and certainties per internal tick, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{L} \in \mathbb{R}^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.880431em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal">L</span></span><span class="mrel">∈</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">C</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{C} \in \mathbb{R}^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.880431em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.05834em;">C</span></span><span class="mrel">∈</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathbb">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</p>
<p>A natural question arises: how should we reduce <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal">L</span></span></span></span></span> into a scalar loss for learning? Our loss function is designed to optimize CTM performance across the internal thought dimension. Instead of relying on a single step (e.g., the last step), which can incentivize the model to only output at that specific step, we dynamically aggregate information from two internal ticks: the point of minimum loss and the point of maximum certainty:</p>
<ul>
<li>the point of minimum loss: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext><mo>(</mo><mrow><mi mathvariant="script">L</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">t_1=\text{argmin}(\mathcal{L})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="text mord textstyle uncramped"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mathrm">m</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span></span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathcal">L</span></span><span class="mclose">)</span></span></span></span>; and</li>
<li>the point of maximum certainty: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub><mo>=</mo><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mtext><mo>(</mo><mrow><mrow><mi mathvariant="script">C</mi></mrow></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">t_2=\text{argmax}({\mathcal{C}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="text mord textstyle uncramped"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.05834em;">C</span></span></span><span class="mclose">)</span></span></span></span>.</li>
</ul>
<p>This approach is advantageous because it means that the CTM can perform meaningful computations across multiple internal ticks, naturally facilitates a curriculum effect, and enables the CTM to tailor computation based on problem difficulty. The final loss is computed as:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow></msup><mo>+</mo><msup><mrow><mi mathvariant="script">L</mi></mrow><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">    L = \frac{\mathcal{L}^{t_1} + \mathcal{L}^{t_2}}{2}.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.470556em;"></span><span class="strut bottom" style="height:2.156556em;vertical-align:-0.686em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">L</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">+</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">L</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mord mathrm">.</span></span></span></span></span></p>
<hr>
<h2>Experiment: ImageNet</h2>
<div class="experiment-card" id="imagenet-experiment">
    <div class="experiment-content"> 
        <div class="experiment-col-1" id='imagenet-demos'>
            <h3>Demonstrations</h3>
            <div class="video-container">
                <video
                    id="imagenet-main-video" class="video-js vjs-default-skin vjs-big-play-centered vjs-fill"
                    preload="auto">
                    <p class="vjs-no-js">Enable JavaScript.</p>
                </video>
            </div>
            <div class="thumbnail-grid" id="imagenet-thumbnail-grid"> </div>
            <figcaption>
                <span class="caption-highlight"><b>Fig 4.</b> Thinking about Images</span>: Top left is the average attention weighting (of the 16 heads shown) when the CTM observes the image on the right. Class predictions are shown on the bottom left and the certainty is shown on the bottom right (<span style="color:rgba(0, 232, 12, 0.7);">green</span> denotes a correct prediction). The small images at the bottom are buttons to load other examples, showing a diversity of certainties and correctness.
            </figcaption>
        </div>
        <div class="experiment-col-2">
            <h3>Results</h3>
            <div class="figure-grid">
                <figure class="grid-figure">
                    <img src="assets/png/imagenet/accuracy_types_5.svg" alt="Accuracy types for ImageNet">
                    <figcaption>
                    <span class="caption-highlight"><b>Fig 5a.</b> Top-5 Accuracies</span>: using different mechanisms for predictions, the CTM achieves different levels of accuracy per internal tick (thought step). At about 15 ticks it makes sense to account for certainty.
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <img src="assets/png/imagenet/imagenet_calibration.svg" alt="Calibration on ImageNet validation set">
                    <figcaption>
                    <span class="caption-highlight"><b>Fig 5b.</b> Calibration</span>: often considered an important measure of how well a model fits the underlying data distribution, the CTM has remarkably good calibration.
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <img src="assets/png/imagenet/steps_versus_correct_0.5.png" alt="Setting a threshold of 0.5 certainty">
                    <figcaption>
                    <span class="caption-highlight"><b>Fig 5c.</b> Certainty threshold=0.5</span>: top-5 accuracy at this certainty threshold (black line, bottom right in the videos to the left).
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <img src="assets/png/imagenet/steps_versus_correct_0.8.png" alt="Setting a threshold of 0.8 certainty">
                    <figcaption>
                    <span class="caption-highlight"><b>Fig 5d.</b> Certainty threshold=0.9</span>: top-5 accuracy at this certainty threshold (black line, bottom right in the videos to the left).
                    </figcaption>
                </figure>
            </div>
            <div>
                <p>This is a subset of results from our ImageNet experiments. Crucially, the CTM enables <b>Adaptive Compute</b> where the internal steps, (<i>how much thought the CTM is putting into the problem</i>) can be cut short. These figures show what can be expected in terms of accuracy when cutting thinking short. Only marginal gains are had past a certain point, but gains nonetheless.</p>
                <p> Fig 4. shows where the CTM looks as it reasons about the data. We show the <b>Attention Weights</b> for all 16 heads and demark where the model is looking for each (and on average at the top). The predictions are shown on the bottom left and certainty over time on the bottom right. Fig 6. shows a visualization of <b>Neural Activity</b> as the CTM thinks about a single image: note the multi-scale structure and how activity seems to 'flow'.
            </div>
            <div id='umap'>
                <video src="assets/mp4/umap.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video"></video>
            </div>
            <figcaption>
                <span class="caption-highlight"><b>Fig 6.</b> Neural activity</span>: visualised in 2D using a <a href="https://umap-learn.readthedocs.io/en/latest/" target="_blank">UMAP</a> projection. Each neuron is shown as an individual dot, scaling in size with absolute magnitude, and color with value (<span style="color:rgba(46, 0, 232, 0.7);">blue</span> for negative, <span style="color:rgba(232, 0, 0, 0.7);">red</span> for positive). We show similar visualizations inside later demonstrations.
            </figcaption>
        </div>
        <div class="experiment-col-3">
            <h3>Discussion</h3>
            <p>We never set out to train a model that achieved some remarkable new state-of-the-art performance on ImageNet. AI researchers already expect high performance on ImageNet after over a decade of research that uses it. Instead, we wanted to show just how different and interesting <b>the CTM's interaction with data</b> can be. The videos on the left/above demonstrate the thought process the CTM undertakes and the figures show its benefits.
            <p>Let's contextualize just what's going on here: the CTM is looking around these images, all the while building up its prediction, all by using the <b>synchronization of neural activity</b> directly as a representation. The <a href="#neural-dynamics">neural dynamics</a> we showed earlier are actually examples of dynamics from a CTM observing ImageNet! The paths output by the CTM in <a href="#maze-demo">the maze demo</a> are akin to the class predictions made here.
            <h4>The missing ingredient: TIME</h4>
            <p><b>Biological intelligence is still superior to AI in many cases</b> <dt-cite key="chollet2024arc,phan2025humanitysexamshort,lake2017building,ren2024brain"></dt-cite>. Biological brains solve tasks very differently to conventional neural networks, which might explain why this is the case. It might be that <a href="https://www.thetransmitter.org/neuroai/the-brain-holds-no-exclusive-rights-on-how-to-create-intelligence/" target="_blank">biological intelligence pays heed to time</a> in ways that modern AI simply does not. In this work, we aimed to develop a model that approaches problem-solving in a manner more aligned with biological brains, emphasizing the central role of the precise timing and interplay of neural dynamics. The interpretable and intuitive outcome we point at in the video demonstrations is very exciting as it suggests that the CTM is indeed leveraging time to its advantage, in order to reason about data.</p>
        </div>
    </div> 
</div>
<script>
    const imagenetVideoList = [ '11037', '12495', '13575', '14308', '17352', '17558', '22403', '23597', '2677',  '27230', '30917', '3415',  '35303', '37164', '4014',  '4225', '42416', '43479', '49506', '7343' ];
    initializeImageNetPlayer(imagenetVideoList, {
        playerElementId: 'imagenet-main-video',      // Use unique ID
        thumbnailGridId: 'imagenet-thumbnail-grid' // Use unique ID
    });
</script>
<h2>Experiment: Solving 2D Mazes - doing it the hard way</h2>
<div class="experiment-card" id="maze-experiment">
    <div class="experiment-content"> 
        <div class="experiment-col-1">
            <h3>The why and the how</h3>
            <p>
            Solving mazes is a challenging task for machines <dt-cite key="zhang2025t,schwarzschild2021can,bansal2022end"></dt-cite>, where only the current <a href="https://openai.com/index/thinking-with-images/" target="_blank">bleeding edge models perform well</a> on fairly simple <a href="https://featurecrew.io/tests/maze" target="_blank">mazes</a>. Even so, existing methods either require careful design of the data/objective (e.g., outputs are images instead of a <i>solution</i>), or extensive tool use (e.g., LLMs that perform well at this), indicating that the underlying <b>intelligent reasoning</b> required to solve a maze, step-by-step, is not evidenced by these approaches.
            </p>
            <p>
            We trained n CTM on a new setup, requiring it to directly predict a path (truncated for simplicity) from start to finish in the form of steps: <b>L</b>eft, <b>R</b>ight, <b>U</b>p, <b>D</b>own, or <b>W</b>ait. A small version of the resultant model can be explored in the <a href="#maze-demo">interactive demo at the top of this page</a>. We show a demonstration of larger model here. Remarkably, the attention pattern is intuitive and follows the solution, all while using neural synchronization as a representation. It even generalizes beyond the truncated path!
            </p>
            <h3>Demonstration</h3>
            <div>
                <video src="assets/mp4/mazes/maze-solve.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video"></video>
            </div>
            <figcaption>
                <span class="caption-highlight"><b>Fig 7.</b> Thinking about mazes</span>: each animation segment shows 75 internal ticks of the CTM when it is provided with the input maze. We show the route as it is constructed through the internal 'thought process', showing only the valid route (i.e., ignoring predictions through walls; see the associated toggle on the <a href="#maze-demo">demo</a>). 16 attention heads' weights are shown at the bottom and the average is overlayed on the maze to show where the CTM is focusing. We 'teleport' the CTM to its resultant predicted location until it lands on the target and then load a new maze.
            </figcaption>
        </div>
        <div class="experiment-col-2">
            <h3 id='maze-results'>Results</h3>
            <div class="figure-grid">
                <figure class="grid-figure">
                    <img src="assets/png/mazes/acc.svg" alt="Accuracy over training run">
                    <figcaption>
                    <span class="caption-highlight"><b>Fig 8a.</b> Accuracy during training</span>: versus the best baselines we could get working. The CTM, <span style="color:rgba(255, 39, 201, 0.7);">shown in pink</span>, gets nearly perfect validation accuracy.
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <img src="assets/png/mazes/accuracy_vs_pathlength.svg" alt="Accuracy versus path length">
                    <figcaption>
                    <span class="caption-highlight"><b>Fig 8b.</b> Accuracy versus path length</span>: the baselines are certainly learning, but the CTM far outperforms them for longer paths.
                    </figcaption>
                </figure>
            </div>
            <h3>Generalization</h3>
            <p>Each video below shows how well the CTM generalizes to bigger and more complex mazes, while retaining its reasoning prowess. To generate these we used a CTM trained to solve a path up to length 100 on 39 x 39 mazes, but the mazes shown here are of size 99 x 99 and the full paths are roughly 6x as long.</p>
            <div class="figure-grid">
                <figure class="grid-figure">
                    <video src="assets/mp4/mazes/gen-1.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video" style="width:95%"></video>
                </figure>
                <figure class="grid-figure">
                    <video src="assets/mp4/mazes/gen-2.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video" style="width:95%"></video>
                </figure>
                <figure class="grid-figure">
                    <video src="assets/mp4/mazes/gen-3.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video" style="width:95%"></video>
                </figure>
                <figure class="grid-figure">
                    <video src="assets/mp4/mazes/gen-4.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video" style="width:95%"></video>
                </figure>
            </div>
        </div>
        <div class="experiment-col-3">
            <h3>Discussion</h3>
            <p>Why run these experiments? We know that neural networks can be tailored to solve 2D mazes if we present the data in the "right" way. But, when presented in a fashion that requires a clear process through which the model must progress, existing methods fall short. Even current SoTA LLMs rely on tool use, which is impressive in its own right, but somewhat unsatisfying: an intelligent machine should be demonstrably intelligent, and humans don't require tools to solve these mazes. </p>
            <p>We set out to show that the CTM has the capacity to learn when complex reasoning is required, unlike the most comparable baseline methods. We also show how the CTM generalizes to larger and more complex mazes, indicating that its internal reasoning is not merely memorization, but rather a more natural and correct way to solve the underlying maze problem. Importantly, we made no specific structural changes to the model compared to the <a href="#imagenet-experiment">CTM we trained for ImageNet</a>; the only meaningful structural change was to output the solution as a 2D class space, applying cross entropy for each step.
            <h4>A World Model</h4>
            <p>We chose our setup carefully: (1) we used <b>no positional embedding</b> for attention; and (2) we required that the models predict the routes directly as a string of classes (e.g., go left, left, right, up, etc.). By forgoing positional embedding the CTM must build an <b>internal world model</b> in order to query the data and navigate the maze. The fact that it does so in such a convincing fashion is remarkable. </p>
            <h4>Where to go from here?</h4>
            <p>We have some strong evidence that the CTM is capable of solving challenging problems, and it does so in intuitive and interesting ways. The fact that it can solve mazes by building an internal world model "on the fly" without any positional embedding opens up avenues for future research. For instance, we would like to see how the CTM finds its way around more complex environments (e.g., games or videos) without any explicit positional encodings.</p>
        </div>
    </div> 
</div>
<h2>Experiment: Parity</h2>
<div class="experiment-card" id="parity-experiment">
    <div class="experiment-content"> 
        <div class="experiment-col-1">
            <h3>Sequential data, non-sequentially</h3>
            <p>
            <p>
            The parity of a binary sequence, given by the sign of the product of its elements, can reasonably be predicted by an RNN when the data is fed sequentially - the model need only maintain an internal state, flipping a 'switch' whenever a negative number if encountered. When the entire sequence is provided at once, however, the task is significantly more challenging<dt-cite key="graves2016adaptive"></dt-cite>.
            </p>
            <p>
            We trained CTMs to solve a variant of this parity task: the model is input with a 64-length binary vector, and must predict the <i>cumulative</i> parity at each of the 64 positions.
            </p>
            <h3>Demonstration</h3>
            <div>
                <video src="assets/mp4/parity/activations.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video"></video>
            </div>
            <figcaption>
                <span class="caption-highlight"><b>Fig 9.</b> Determining the cumulative parity of a sequence</span>: shown are the movements of the attention weights from each of the 8 heads. Overlayed on the input sequences is the trajectory of the attention weight argmax. The larger sequences depict the models predictions and targets.
            </figcaption>
        </div>
        <div class="experiment-col-2">
            <h3>Results</h3>
            <div class="figure-grid" style="display: flex; gap: 16px; align-items: flex-start; width: 100%;">
                <figure class="grid-figure" style="flex: 0 0 64%; margin: 0;">
                    <img src="assets/png/parity/accuracy_comparison.svg" alt="Accuracy during training" style="width: 100%; height: auto; display: block;">
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 10a.</b> Accuracy during training</span>: versus the LSTMs, averaged over 3 training runs. The best, <span style="color:#5f57db;"> 75-iteration model</span>, achieves perfect accuracy in some runs.
                    </figcaption>
                </figure>
                <figure class="grid-figure" style="flex: 0 0 32%; margin: 0;">
                    <img src="assets/png/parity/accuracy_vs_thinking_time.svg" alt="Accuracy versus path length" style="width: 100%; height: auto; display: block;">
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 10b.</b> Accuracy versus thinking time</span>: more internal ticks leads to higher accuracy.
                    </figcaption>
                </figure>
                </div>
                <p>
                We compare the accuracy of CTMs trained with different numbers of internal ticks to parameter matched LSTMs. We found that CTMs with over 75 internal ticks could reliably solve this task, with some runs achieving 100% accuracy. The LSTMs, on the other hand, struggled to learn with over 10 internal ticks, suggesting that LSTMs are not well suited to unfolding an internal thought dimension.
                </p>
                <p>
                The left/above demonstration shows the solving process of the CTM: the movement of the attention weights, as well as their argmax overlayed on the inputs, the models predictions, the target and the neuron activations. Notice how the attention moves <b>backwards</b> through the data and determines the solution after observing the entire input. Some attention heads display interpretable behavior, such as the first attention head which attends to only negative parity positions (\(\blacksquare\)).
                </p>
            </div>
        <div class="experiment-col-3">
   <h3>Learning sequential algorithms</h3>
            <p>We visualise the learned algorithms by plotting the accuracy (top) and attention weights (bottom) over the 75 internal ticks for each position in the 64-length sequence, at different points during training. One model (left) attends to the data in reverse order before predicting the cumulative parity at once; the other attends forward, predicting parity incrementally. Both achieve perfect accuracy.
            </p>
            <p>
            The ability of the CTM to search through the data in reverse order, suggests that the CTM is carrying out some form of planning, building up its understanding of the data before making a final decision -- the CTM is capable of forming and following a strategy.
            </p>
            <div class="figure-grid">
                <figure class="grid-figure">
                    <video src="assets/mp4/parity/run1.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video" style="width:100%"></video>
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 11a.</b> 75-Internal Tick CTM 1</span>: learns to attend to the data in reverse order, predicting the parity at the end of the reasoning process.
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <video src="assets/mp4/parity/run3.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video" style="width:100%"></video>
                        <figcaption>
                        <span class="caption-highlight"><b>Fig 11b.</b> 75-Internal Tick CTM 2</span>: learns to attend from beginning to end, and with it, increasing its certainty in each prediction.
                    </figcaption>
                </figure>
            </div>
        </div>
    </div> 
</div>
<h2>Experiment: Q&amp;A MNIST</h2>
<div class="experiment-card" id="parity-experiment">
    <div class="experiment-content"> 
        <div class="experiment-col-1">
            <h3>Memory via Synchronization</h3>
            <p>
            <p>
            To assess the CTM’s ability to memorise and recall information, we design a Question and Answering (Q&A) MNIST task. In this task, the model first observes a sequence of MNIST digits, followed by a series of interleaved index and operator embeddings that specify which digits should be recalled and which modular operation should be applied. Once all digits and index/operator embeddings have been presented, a zero-tensor flag signals the model to produce its final answer. An example is shown below.
            </p>
            <figure class="grid-figure" style="flex: 0 0 64%; margin: 0;">
                <img src="assets/png/qamnist/qamnist_example.svg" alt="Accuracy during training" style="width: 100%; height: auto; display: block;">
                <figcaption>
                    <span class="caption-highlight"><b>Fig 12.</b> Q&A MNIST example</span>: a typical sequence observed by the model.
                </figcaption>
            </figure>
            <p>
            In our experiments, the memory length of the CTMs is such that the MNIST digits will always lie outside of the activation history window used by the neuron-level models. In this way, the CTM must organize its activations such that it can recall digits are later timesteps.
            </p>
            <h3>Demonstration</h3>
            <div>
                <video src="assets/mp4/qamnist/example.mp4" type="video/mp4" autoplay muted playsinline loop class="centered-video"></video>
            </div>
            <figcaption>
                <span class="caption-highlight"><b>Fig 13.</b>Observing digits and answering questions</span>: the model is shown MNIST digits followed by operator and index embeddings which specifies the modular operation at the top. Shown also is the attention weights for the digits and the models predictions.
            </figcaption>
        </div>
        <div class="experiment-col-2">
            <h3>Results</h3>
            <div class="figure-grid" style="display: flex; gap: 16px; align-items: flex-start; width: 100%;">
                <figure class="grid-figure">
                    <img src="assets/png/qamnist/accuracy_comparison.svg" alt="Accuracy during training" style="width: 100%; height: auto; display: block;">
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 14.</b> Accuracy during training</span>: for both CTMs and LSTMs trained with 1 internal tick per input and 10 internal ticks per input.
                    </figcaption>
                </figure>
                </div>
                <p>
                Our results show that, while the LSTM outperforms the CTM when only a single internal tick is used to process each input, the LSTM becomes more unstable when more internal ticks are used. The CTM, on the other hand, exhibits stronger performance with increasing internal ticks, achieving over 95% accuracy in the most challenging in-distribution task. 
                </p>
                <p>
                Furthermore, we highlight the ability of the CTM to recall digit values observed many timesteps in the past, arising purely from the organization and synchronization of neurons. This strong performance suggests that processing timing information through the synchronization of neuron activations may be a powerful mechanism for memorization and recall.
                </p>
            </div>
        <div class="experiment-col-3">
            <h3>Generalization</h3>
            <p>
            We examine the generalization capabilities of the CTM by measuring the accuracy of the model when input with more digits or index-operator embeddings than observed during training, depicted below, with the training regime marked in red. We find that both the CTM and the LSTM baseline can generalize to an increased number of operations. Empirically, we find that this generalization arises from the model’s approach to solving the task: each time a new index embedding is presented, the model computes and stores the result of the specified operation, regardless of whether the answer flag has been given. This enables it to continue processing a stream of index and operator embeddings without needing to wait for a final signal.
            </p>
            <div class="figure-grid" style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px;">
                <figure class="grid-figure">
                    <img src="assets/png/qamnist/accuracy_grid_atm_1.png" alt="Accuracy during training" style="width: 100%; height: auto; display: block;">
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 15a.</b> CTM, 1 internal tick</span>
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <img src="assets/png/qamnist/accuracy_grid_lstm_1.png" alt="Accuracy during training" style="width: 100%; height: auto; display: block;">
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 15b.</b> LSTM, 1 internal tick</span>
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <img src="assets/png/qamnist/accuracy_grid_atm_10.png" alt="Accuracy during training" style="width: 100%; height: auto; display: block;">
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 15c.</b> CTM, 10 internal ticks</span>
                    </figcaption>
                </figure>
                <figure class="grid-figure">
                    <img src="assets/png/qamnist/accuracy_grid_lstm_10.png" alt="Accuracy during training" style="width: 100%; height: auto; display: block;">
                    <figcaption>
                        <span class="caption-highlight"><b>Fig 15d.</b> LSTM, 10 internal ticks</span>
                    </figcaption>
                </figure>
            </div>
            <figcaption style="margin-top: 12px;">
                <span class="caption-highlight"><b>Fig 15.</b> Generalization:</span> accuracy of the CTM and LSTM for different numbers of input digits and operations. The red line indicates the training regime. For the CTM, performance scales with the number of internal ticks, while the converse is true for the LSTM.
            </figcaption>
        </div>
        </div>
    </div> 
</div>
<hr>
<h2>Additional experiments</h2>
<h3>CTM versus humans</h3>
<p>In this section we test the CTM using CIFAR-10, comparing it to human performance, a feed-forward baseline, and an LSTM baseline. The purpose of this experiment was to contextualize the performance of the CTM alongside a standard feed-forward baseline, an LSTM baseline that also uses internal ticks for reasoning (potentially), and humans. We used a restricted backbone to highlight the differences between models.</p>
<p>We used two datasets of human labels for CIFAR-10; we call these CIFAR-10D <dt-cite key="ho2018cifar10"></dt-cite> owing to its calibration of difficulty levels, and CIFAR-10H <dt-cite key="peterson2019human"></dt-cite> originally used to quantify human uncertainty. CIFAR-10D can be found at <a href="https://sites.google.com/site/hophuoctien/projects/virec/cifar10-classification" target="_blank">here</a> and CIFAR-10H can be found <a href="https://github.com/jcpeterson/cifar-10h" target="_blank">here</a>.</p>
<div class="figure-grid-container">
  <figure class="grid-item grid-item-span-2">
    <img src="assets/png/cifar10/training_curves.svg" alt="Accuracy curves during training">
    <figcaption>
    <span class="caption-highlight"><b>Fig16a.</b> Accuracy curves during training</span>: using parameter-matched models, the CTM generalizes best. One of the seeds had lower accuracy initially but it recovered and outperformed, interestingly, outperformed all others. 
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/cifar10/calibration.svg" alt="Calibration plots">
    <figcaption>
    <span class="caption-highlight"><b>Fig 16b.</b> Calibration plots</span>: for all models and humans. We show calibration at each step of thought for the CTM, taking the average probability up to that step for computing these. The CTM even has superior calibration than humans. 
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/cifar10/difficulty_vs_accuracy.svg" alt="CIFAR-10D difficulty plots">
    <figcaption>
    <span class="caption-highlight"><b>Fig 16c.</b> CIFAR-10D difficulty plots</span>: showing how the CTM performs best at predicting difficult classes, perhaps benefiting from additional "time to think".
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/cifar10/reaction_times_lstm.svg" alt="LSTM pseudo 'reaction times'">
    <figcaption>
    <span class="caption-highlight"><b>Fig 16d.</b> LSTM pseudo "reaction times"</span>: computed as (1 - the average certainty) over internal ticks, measured against real human reaction times from CIFAR-10H.
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/cifar10/reaction_times_CTM.svg" alt="CTM pseudo 'reaction times'">
    <figcaption>
    <span class="caption-highlight"><b>Fig 16e.</b> CTM pseudo "reaction times"</span>: while not any 'better' than the LSTM, this shows an interesting pattern where the CTM reacts more 'quickly' to challenging data.
    </figcaption>
  </figure>
</div>
<p>For the human calibration we used the probabilities provided in CIFAR-10H, which were computed using guesses from multiple humans using the available human datasets. We computed calibration (Fig 16b.) as we did for ImageNet: we compute the predictive probability as the average probability for the chosen class over all internal ticks (for both CTM and LSTM). The CTM demonstrates the best calibration, even when compared to humans.</p>
<div class="figure-grid" style="max-width:800px">
    <figure class="grid-figure">
        <img src="assets/png/cifar10/dynamics_atm.png" alt="Neural dynamics of the CTM">
    </figure>
    <figure class="grid-figure">
        <img src="assets/png/cifar10/dynamics_lstm.png" alt="Neural dynamics of the LSTM">
    </figure>
    <figure class="grid-figure" style="grid-column: span 2; ">
        <figcaption>
        <span class="caption-highlight"><b>Fig 17.</b> CTM (left) and LSTM (right) neural dynamics</span>: over internal (50) ticks. We show dynamics from other data points in the background to show how diverse these can befor the CTM. The dot products between pairs of vectors like these (not necessarily exactly these ones) is <b>Synchronization</b> and that is the representation used to predict the classes by the CTM.
        </figcaption>
    </figure>
</div>
<p>Fig 17. shows the neural activities for the CTM and the LSTM baseline. The CTM yields rich, diverse, and complex dynamics with multiple interesting features, including periodic behavior (there is <b>no periodic driving function</b>). The distinct difference between the CTM and LSTM neural activities is evidence that the two novel elements of the CTM (<a href="#neuron-level-models">neuron-level models</a> and <a href='#synchronization-representation'>synchronization as a representation</a>) enable neural dynamics as a fundamental computational mechanic.</p>
<h3>CIFAR-100, ablation studies</h3>
<p>Fig 18. shows what happens when we vary the number of neurons (i.e., the model width) while keeping all else constant, including the training time. As with other models, a wider network could evidently benefit from a longer training time or different training hyper-parameters, hence the reduction in accuracy in Fig 18a. For Fig 18b. and Fig 18c. we set out to understand how unique the <a href='#neuron-level-models'>Neuron-level models</a> tend to be, and that was related to the model width, as measured by the cosine similarity between the dynamics of different neurons. Fig 18b. shows that with a wider model (i.e., more neurons), we see more diversity instead of less. One might expect that with more neurons there is less 'space' for diversity, but we observed the opposite.</p>
<div class="figure-grid-container">
  <figure class="grid-item">
    <img src="assets/png/cifar100/ablation_width_acc.svg" alt="Accuracy versus model width">
    <figcaption>
    <span class="caption-highlight"><b>Fig 18a.</b> Accuracy versus model width</span>: when trained on CIFAR-100. Each model had equal training, indicating that the wider models could benefit from more training. 
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/cifar100/ablation_width_overdata.png" alt="Neuron similarity across data">
    <figcaption>
    <span class="caption-highlight"><b>Fig 18b.</b> Neuron similarity across data</span>: averaged over all neurons, showing how a wider model yields more diverse neurons instead of more overlap (which might be expected). 
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/cifar100/ablation_width_internal.png" alt="Neuron similarity across neurons">
    <figcaption>
    <span class="caption-highlight"><b>Fig 18c.</b> Neuron similarity across neurons</span>: averaged over data, showing a slightly reduced similarity for wider models.
    </figcaption>
  </figure>
</div>
<p>Fig 19. shows the relationship between predictions and the number of internal ticks used by the CTM. We trained several CTMs (again keeping all other variables constant). In Fig 19b. we plot the distributions of the data over which steps the CTM is most certain (i.e., <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">t_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> in <a href="#loss-function">the loss function</a>). What this shows is that the CTM uses a wide range of steps to become most certain about the data it observes. For each setup (25, 50 and 100 internal ticks), there are two concentrated areas in the distributions, indicating that the CTM is following separate internal processes depending on the data.</p>
<div class="figure-grid-container">
  <figure class="grid-item ">
    <img src="assets/png/cifar100/ablation_steps_acc.svg" alt="Accuracy versus internal ticks">
    <figcaption>
    <span class="caption-highlight"><b>Fig 19a.</b> Accuracy versus internal ticks</span>: evidencing that more internal ticks might benefit from longer learning.
    </figcaption>
  </figure>
  <figure class="grid-item grid-item-span-2">
    <img src="assets/png/cifar100/ablation_steps_histogram.png" alt="Histogram of most certain indices">
    <figcaption>
    <span class="caption-highlight"><b>Fig 19b.</b>Histogram of most certain indices</span>: for models trained using 25, 50, and 100 internal ticks. In each case there is a double 'hump' in the distributions of certainties, meaning that the CTM might be following two different internal processes depending on the data.
    </figcaption>
  </figure>
</div>
<h3>Sorting real numbers</h3>
<p>For these experiments we trained a CTM to sort 30 real numbers from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msub><mrow><mi>I</mi></mrow><mrow><mn>3</mn><mn>0</mn></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, {I}_{30})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">3</span><span class="mord mathrm">0</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>. The purpose of this experiment was twofold: (1) to understand if and when the CTM applies more or less compute in a controlled environment; and (2) see if we can train the CTM to output a sequence in sequential order using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html">CTC loss</a>. This CTM could sort a length 30 list of real numbers approximately 80% of the time.</p>
<div class="figure-grid-container">
  <figure class="grid-item">
    <img src="assets/png/sort/mean_wait_times.svg" alt="Mean wait times per sequence index">
    <figcaption>
    <span class="caption-highlight"><b>Fig 20a.</b> Mean wait times per sequence index</span>: measured as internal ticks, showing an intersting emergent behavior where the CTM first waits (i.e., does internal compute) before outputting consistently before waiting near the end again.
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/sort/waittimes_vs_delta.jpeg" alt="Wait times versus gap to previous item">
    <figcaption>
    <span class="caption-highlight"><b>Fig 20b.</b> Wait times versus gap to previous item</span>: showing the relationship between how much compute the CTM applies compared to the gap between sorted items.
    </figcaption>
  </figure>
  <figure class="grid-item">
    <img src="assets/png/sort/acc_vs_stddev.svg" alt="Generalizing beyond training distribution">
    <figcaption>
    <span class="caption-highlight"><b>Fig 20c.</b> Generalizing beyond training distribution</span>: showing sorting performance for different Gaussian distributions (it was trained using a Normal distribution).
    </figcaption>
  </figure>
  <figure class="grid-item grid-item-span-3"> 
    <img src="assets/png/sort/wait_use_case_2.svg" alt="Sorting demonstration'">
    <figcaption>
    <span class="caption-highlight"><b>Fig 20d.</b> Sorting demonstration</span>: showing the delta from mean of wait times for each item (plotted in sorted order, color denoting original order using a rainbow colormap). The CTM tends to require more compute when there is a larger gap between points. 
    </figcaption>
  </figure>
</div>
<h3>Reinforcement Learning</h3>
<p>We have shown that the CTM can process non-sequential data via an continuous thought dimension. Here, we extend the CTM to tasks involving interation with an external environment, training CTMs with proximal policy optimization<dt-cite key="schulman2017proximal"></dt-cite> to solve a navigation task and partially observable variants of CartPole and Acrobot<dt-cite key="MinigridMiniworld23,towers2024gymnasium"></dt-cite>. In this setting, the CTM receives an observation, process it using a fixed number of internal thought steps, and outputs the next action. The history of activations is continuous across environment steps, such that activations from past environment steps can affect the present decision making process.</p>
<div class="figure-grid-container">
  <figure class="grid-item ">
    <video src="assets/mp4/rl/activations.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%; height: auto; display: block;"></video>
    <figcaption>
    <span class="caption-highlight"><b>Fig 21a.</b> CTM solving the MiniGrid Four Rooms task</span>: evidencing that the CTM can use a leverage a continuous history of activations to interact with the world.
    </figcaption>
  </figure>
  <figure class="grid-item grid-item-span-2">
    <img src="assets/png/rl/episode_lengths_avg.png" alt="CTM Training Curves of MiniGrid Four Rooms" style="width: 100%; height: auto; display: block;">
    <figcaption>
    <span class="caption-highlight"><b>Fig 21b.</b>Training curves</span>: for this navigation task (episode length during training). Although the LSTM learns slightly faster, both solve the task and converge to the same average episode length.
    </figcaption>
  </figure>
</div>
<p>Although our results show that the CTM achieves a comparable performance to the LSTM baseline, the central goal of this section is provide evidence that the CTM can learn in a continuous environment.</p>
<hr>
<h2>Conclusion</h2>
<p>The Continuous Thought Machine (CTM) represents a novel step towards bridging computational efficiency with biological plausibility in artificial intelligence. By moving beyond traditional pointwise activation functions to private neuron-level models, the CTM cultivates far richer neuron dynamics. Crucially, it leverages neural synchronization as a powerful and fundamentally new type of representation - distinct from the activation vectors prevalent since the early days of neural networks. This direct use of neuron dynamics as a first-class representational citizen allows the CTM to exhibit behaviors qualitatively different from contemporary models.</p>
<p>Our research demonstrates the tangible benefits of this approach. The CTM can dynamically build representations over time for tasks like image classification, form rich internal maps to attend to specific input data without positional embeddings, and naturally exhibit adaptive computation. Furthermore, it learns to synchronize neural dynamics to store and retrieve memories beyond its immediate activation history. This internal processing also lends itself to greater interpretability, as seen in its methodical solving of mazes and parity tasks.</p>
<p>Remarkably, the core CTM architecture remained largely consistent across a diverse range of challenging tasks, requiring only input/output module adjustments. This versatility and trainability were particularly evident in complex scenarios like maze navigation. The CTM succeeded with minimal tuning, where a traditional model like the LSTMs still struggled even after significant tuning efforts.</p>
<p>This work underscores a vital, yet often underexplored, synergy between neuroscience and machine learning. While modern AI is ostensibly brain-inspired, the two fields often operate in surprising isolation. The CTM serves as a testament to the power of drawing inspiration from biological principles. By starting with such inspiration and iteratively following the emergent, interesting behaviors, we developed a model with unexpected capabilities, such as its surprisingly strong calibration in classification tasks, a feature that was not explicitly designed for.</p>
<p>It is crucial to note that our approach advocates for borrowing concepts from biology rather than insisting on strict, literal plausibility; real neurons may not access their activation history as modeled in the CTM, yet emergent phenomena like traveling waves still manifest. This nuanced balance between practicality and biological inspiration opens a landscape of new research directions, which may hold the key to unlocking capabilities currently missing in AI, potentially leading to systems that exhibit more human-like intelligence and address its current limitations.</p>
<p>When we initially asked, &quot;why do this research?&quot;, we hoped the journey of the CTM would provide compelling answers. By embracing light biological inspiration and pursuing the novel behaviors observed, we have arrived at a model with emergent capabilities that exceeded our initial designs. We are committed to continuing this exploration, borrowing further concepts to discover what new and exciting behaviors will emerge, pushing the boundaries of what AI can achieve.</p>
</dt-article>
<dt-appendix>
<h2>Acknowledgements</h2>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
    @inproceedings{guo2017calibration,
        title={On calibration of modern neural networks},
        author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
        booktitle={International conference on machine learning},
        pages={1321--1330},
        year={2017},
        organization={PMLR}
      }
      @article{banino2021pondernet,
        title={Pondernet: Learning to ponder},
        author={Banino, Andrea and Balaguer, Jan and Blundell, Charles},
        journal={arXiv preprint arXiv:2107.05407},
        year={2021}
      }
      @inproceedings{bolukbasi2017adaptive,
        title={Adaptive neural networks for efficient inference},
        author={Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
        booktitle={International Conference on Machine Learning},
        pages={527--536},
        year={2017},
        organization={PMLR}
      }
      @article{gornet2024automated,
        title={Automated construction of cognitive maps with visual predictive coding},
        author={Gornet, James and Thomson, Matt},
        journal={Nature Machine Intelligence},
        volume={6},
        number={7},
        pages={820--833},
        year={2024},
        publisher={Nature Publishing Group UK London}
      }
      
      @article{ha2018world,
        title={World models},
        author={Ha, David and Schmidhuber, J{\"u}rgen},
        journal={arXiv preprint arXiv:1803.10122},
        year={2018}
      }
      
      @article{lecun2022path,
        title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
        author={LeCun, Yann},
        journal={Open Review},
        volume={62},
        number={1},
        pages={1--62},
        year={2022}
      }
      
      @article{geiping2025scaling,
        title={Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach},
        author={Geiping, Jonas and McLeish, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom},
        journal={arXiv preprint arXiv:2502.05171},
        year={2025}
      }
      @inproceedings{ronneberger2015u,
        title={U-net: Convolutional networks for biomedical image segmentation},
        author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
        booktitle={Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
        pages={234--241},
        year={2015},
        organization={Springer}
      }
      @article{stan2024learning,
        title={Learning long sequences in spiking neural networks},
        author={Stan, Matei-Ioan and Rhodes, Oliver},
        journal={Scientific Reports},
        volume={14},
        number={1},
        pages={21957},
        year={2024},
        publisher={Nature Publishing Group UK London}
      }
      @inproceedings{hasani2021liquid,
        title={Liquid time-constant networks},
        author={Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
        booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
        volume={35},
        number={9},
        pages={7657--7666},
        year={2021}
      }
      @article{chahine2023robust,
        title={Robust flight navigation out of distribution with liquid neural networks},
        author={Chahine, Makram and Hasani, Ramin and Kao, Patrick and Ray, Aaron and Shubert, Ryan and Lechner, Mathias and Amini, Alexander and Rus, Daniela},
        journal={Science Robotics},
        volume={8},
        number={77},
        pages={eadc8892},
        year={2023},
        publisher={American Association for the Advancement of Science}
      }
      
      @article{jaeger2007echo,
        title={Echo state network},
        author={Jaeger, Herbert},
        journal={scholarpedia},
        volume={2},
        number={9},
        pages={2330},
        year={2007}
      }
      
      @inproceedings{jaegle2021perceiver,
        title={Perceiver: General perception with iterative attention},
        author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
        booktitle={International conference on machine learning},
        pages={4651--4664},
        year={2021},
        organization={PMLR}
      }
      
      @article{mcinnes2018umap,
        title={Umap: Uniform manifold approximation and projection for dimension reduction},
        author={McInnes, Leland and Healy, John and Melville, James},
        journal={arXiv preprint arXiv:1802.03426},
        year={2018}
      }
      @inproceedings{xue2023adaptive,
        title={Adaptive computation with elastic input sequence},
        author={Xue, Fuzhao and Likhosherstov, Valerii and Arnab, Anurag and Houlsby, Neil and Dehghani, Mostafa and You, Yang},
        booktitle={International Conference on Machine Learning},
        pages={38971--38988},
        year={2023},
        organization={PMLR}
      }
      @article{goyal2019recurrent,
        title={Recurrent independent mechanisms},
        author={Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
        journal={arXiv preprint arXiv:1909.10893},
        year={2019}
      }
      @article{zelikman2024quiet,
        title={Quiet-star: Language models can teach themselves to think before speaking},
        author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
        journal={arXiv preprint arXiv:2403.09629},
        year={2024}
      }
      @article{tan2023sparse,
        title={Sparse universal transformer},
        author={Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
        journal={arXiv preprint arXiv:2310.07096},
        year={2023}
      }
      @article{graves2016adaptive,
        title={Adaptive computation time for recurrent neural networks},
        author={Graves, Alex},
        journal={arXiv preprint arXiv:1603.08983},
        year={2016}
      }
      
      @article{miyato2024artificial,
        title={Artificial Kuramoto Oscillatory Neurons},
        author={Miyato, Takeru and L{\"o}we, Sindy and Geiger, Andreas and Welling, Max},
        journal={arXiv preprint arXiv:2410.13821},
        year={2024}
      }
      
      
      @article{schwarzschild2021can,
        title={Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks},
        author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
        journal={Advances in Neural Information Processing Systems},
        volume={34},
        pages={6695--6706},
        year={2021}
      }
      
      @misc{maze-dataset,
          title={A Configurable Library for Generating and Manipulating Maze Datasets}, 
          author={Michael Igorevich Ivanitskiy and Rusheb Shah and Alex F. Spies and Tilman Räuker and Dan Valentine and Can Rager and Lucia Quirke and Chris Mathwin and Guillaume Corlouer and Cecilia Diniz Behn and Samy Wu Fung},
          year={2023},
          eprint={2309.10498},
          archivePrefix={arXiv},
          primaryClass={cs.LG},
          url={http://arxiv.org/abs/2309.10498}
      }
      
      @article{marcus2018deep,
        title={Deep learning: A critical appraisal},
        author={Marcus, Gary},
        journal={arXiv preprint arXiv:1801.00631},
        year={2018}
      }

      @article{maass2001relevance,
        title={On the relevance of time in neural computation and learning},
        author={Maass, Wolfgang},
        journal={Theoretical Computer Science},
        volume={261},
        number={1},
        pages={157--178},
        year={2001},
        publisher={Elsevier}
      }

      @article{chollet2019measure,
        title={On the measure of intelligence},
        author={Chollet, Fran{\c{c}}ois},
        journal={arXiv preprint arXiv:1911.01547},
        year={2019}
      }

      @article{cariani2022time,
        title={Time is of the essence: neural codes, synchronies, oscillations, architectures},
        author={Cariani, Peter and Baker, Janet M},
        journal={Frontiers in Computational Neuroscience},
        volume={16},
        pages={898829},
        year={2022},
        publisher={Frontiers Media SA}
      }
      
      
      @article{zhang2025t,
        title={T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model},
        author={Zhang, Tao and Pan, Jia-Shu and Feng, Ruiqi and Wu, Tailin},
        journal={arXiv preprint arXiv:2502.01989},
        year={2025}
      }
      
      
      @article{bansal2022end,
        title={End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking},
        author={Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
        journal={Advances in Neural Information Processing Systems},
        volume={35},
        pages={20232--20242},
        year={2022}
      }
      
      
      
      @article{ho2018cifar10,
        title={CIFAR10 to compare visual recognition performance between deep neural networks and humans},
        author={Ho-Phuoc, Tien},
        journal={arXiv preprint arXiv:1811.07270},
        year={2018}
      }
      
      @inproceedings{peterson2019human,
        title={Human uncertainty makes classification more robust},
        author={Peterson, Joshua C and Battleday, Ruairidh M and Griffiths, Thomas L and Russakovsky, Olga},
        booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
        pages={9617--9626},
        year={2019}
      }
      
      @article{barto1983neuronlike,
        title={Neuronlike adaptive elements that can solve difficult learning control problems},
        author={Barto, Andrew G and Sutton, Richard S and Anderson, Charles W},
        journal={IEEE transactions on systems, man, and cybernetics},
        number={5},
        pages={834--846},
        year={1983},
        publisher={IEEE}
      }
      
      @article{sutton1995generalization,
        title={Generalization in reinforcement learning: Successful examples using sparse coarse coding},
        author={Sutton, Richard S},
        journal={Advances in neural information processing systems},
        volume={8},
        year={1995}
      }
      
      @article{MinigridMiniworld23,
        author       = {Maxime Chevalier-Boisvert and Bolun Dai and Mark Towers and Rodrigo de Lazcano and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
        title        = {Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
        journal      = {CoRR},
        volume       = {abs/2306.13831},
        year         = {2023},
      }
      
      @misc{towers2024gymnasium,
            title={Gymnasium: A Standard Interface for Reinforcement Learning Environments}, 
            author={Mark Towers and Ariel Kwiatkowski and Jordan Terry and John U. Balis and Gianluca De Cola and Tristan Deleu and Manuel Goulão and Andreas Kallinteris and Markus Krimmel and Arjun KG and Rodrigo Perez-Vicente and Andrea Pierré and Sander Schulhoff and Jun Jet Tai and Hannah Tan and Omar G. Younis},
            year={2024},
            eprint={2407.17032},
            archivePrefix={arXiv},
            primaryClass={cs.LG},
            url={https://arxiv.org/abs/2407.17032}, 
      }
      
      @inproceedings{hausknecht2015deep,
        title={Deep Recurrent Q-Learning for Partially Observable MDPs.},
        author={Hausknecht, Matthew J and Stone, Peter},
        booktitle={AAAI fall symposia},
        volume={45},
        pages={141},
        year={2015}
      }
      
      @article{schulman2017proximal,
        title={Proximal policy optimization algorithms},
        author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
        journal={arXiv preprint arXiv:1707.06347},
        year={2017}
      }
      
      @article{lecun1998gradient,
        title={Gradient-based learning applied to document recognition},
        author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
        journal={Proceedings of the IEEE},
        volume={86},
        number={11},
        pages={2278--2324},
        year={1998},
        publisher={Ieee}
      }
      @book{hebb2005organization,
        title={The organization of behavior: A neuropsychological theory},
        author={Hebb, Donald Olding},
        year={2005},
        publisher={Psychology press}
      }
      @article{frankle2018lottery,
        title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
        author={Frankle, Jonathan and Carbin, Michael},
        journal={arXiv preprint arXiv:1803.03635},
        year={2018}
      }
      @inproceedings{allen2019convergence,
        title={A convergence theory for deep learning via over-parameterization},
        author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
        booktitle={International conference on machine learning},
        pages={242--252},
        year={2019},
        organization={PMLR}
      }
      
      @article{schmidgall2024brain,
        title={Brain-inspired learning in artificial neural networks: a review},
        author={Schmidgall, Samuel and Ziaei, Rojin and Achterberg, Jascha and Kirsch, Louis and Hajiseyedrazi, S and Eshraghian, Jason},
        journal={APL Machine Learning},
        volume={2},
        number={2},
        year={2024},
        publisher={AIP Publishing}
      }
      @article{wang2024comprehensive,
        title={A comprehensive survey of continual learning: Theory, method and application},
        author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
        journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
        year={2024},
        publisher={IEEE}
      }
      @article{kudithipudi2022biological,
        title={Biological underpinnings for lifelong learning machines},
        author={Kudithipudi, Dhireesha and Aguilar-Simon, Mario and Babb, Jonathan and Bazhenov, Maxim and Blackiston, Douglas and Bongard, Josh and Brna, Andrew P and Chakravarthi Raja, Suraj and Cheney, Nick and Clune, Jeff and others},
        journal={Nature Machine Intelligence},
        volume={4},
        number={3},
        pages={196--210},
        year={2022},
        publisher={Nature Publishing Group UK London}
      }
      @article{najarro2020meta,
        title={Meta-learning through hebbian plasticity in random networks},
        author={Najarro, Elias and Risi, Sebastian},
        journal={Advances in Neural Information Processing Systems},
        volume={33},
        pages={20719--20731},
        year={2020}
      }
      @article{manhaeve2018deepproblog,
        title={Deepproblog: Neural probabilistic logic programming},
        author={Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
        journal={Advances in neural information processing systems},
        volume={31},
        year={2018}
      }
      @article{caporale2008spike,
        title={Spike timing--dependent plasticity: a Hebbian learning rule},
        author={Caporale, Natalia and Dan, Yang},
        journal={Annu. Rev. Neurosci.},
        volume={31},
        number={1},
        pages={25--46},
        year={2008},
        publisher={Annual Reviews}
      }
      @article{wei2022emergent,
        title={Emergent abilities of large language models},
        author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
        journal={arXiv preprint arXiv:2206.07682},
        year={2022}
      }
      @article{lecun2015deep,
        title={Deep learning},
        author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
        journal={nature},
        volume={521},
        number={7553},
        pages={436--444},
        year={2015},
        publisher={Nature Publishing Group UK London}
      }
      
      @book{goodfellow2016deep,
        title={Deep learning},
        author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
        volume={1},
        number={2},
        year={2016},
        publisher={MIT press Cambridge}
      }
      
      @article{hochreiter1997long,
        title={Long short-term memory},
        author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
        journal={Neural computation},
        volume={9},
        number={8},
        pages={1735--1780},
        year={1997},
        publisher={MIT press}
      }
      @inproceedings{dey2017gate,
        title={Gate-variants of gated recurrent unit (GRU) neural networks},
        author={Dey, Rahul and Salem, Fathi M},
        booktitle={2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)},
        pages={1597--1600},
        year={2017},
        organization={IEEE}
      }
      @book{medsker1999recurrent,
        title={Recurrent neural networks: design and applications},
        author={Medsker, Larry and Jain, Lakhmi C},
        year={1999},
        publisher={CRC press}
      }
      
      @article{vaswani2017attention,
        title={Attention is all you need},
        author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
        journal={Advances in neural information processing systems},
        volume={30},
        year={2017}
      }
      
      @inproceedings{he2016deep,
        title={Deep residual learning for image recognition},
        author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
        booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
        pages={770--778},
        year={2016}
      }
      
      @article{atance2001episodic,
        title={Episodic future thinking},
        author={Atance, Cristina M and O'Neill, Daniela K},
        journal={Trends in cognitive sciences},
        volume={5},
        number={12},
        pages={533--539},
        year={2001},
        publisher={Elsevier}
      }
      
      @inproceedings{pedersen2024structurally,
        title={Structurally Flexible Neural Networks: Evolving the Building Blocks for General Agents},
        author={Pedersen, Joachim and Plantec, Erwan and Nisioti, Eleni and Montero, Milton and Risi, Sebastian},
        booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
        pages={1119--1127},
        year={2024}
      }
      
      @inproceedings{kirsch2022introducing,
        title={Introducing symmetries to black box meta reinforcement learning},
        author={Kirsch, Louis and Flennerhag, Sebastian and Van Hasselt, Hado and Friesen, Abram and Oh, Junhyuk and Chen, Yutian},
        booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
        volume={36},
        number={7},
        pages={7202--7210},
        year={2022}
      }
      
      @article{kirsch2021meta,
        title={Meta learning backpropagation and improving it},
        author={Kirsch, Louis and Schmidhuber, J{\"u}rgen},
        journal={Advances in Neural Information Processing Systems},
        volume={34},
        pages={14122--14134},
        year={2021}
      }
      
      
      
      
      
      @inproceedings{graves2006connectionist,
        title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
        author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
        booktitle={Proceedings of the 23rd international conference on Machine learning},
        pages={369--376},
        year={2006}
      }
      
      
      
      
      
      @article{uhlhaas2009neural,
        title={Neural synchrony in cortical networks: history, concept and current status},
        author={Uhlhaas, Peter and Pipa, Gordon and Lima, Bruss and Melloni, Lucia and Neuenschwander, Sergio and Nikoli{\'c}, Danko and Singer, Wolf},
        journal={Frontiers in integrative neuroscience},
        volume={3},
        pages={543},
        year={2009},
        publisher={Frontiers}
      }
      
      
      @article{chollet2024arc,
        title={Arc prize 2024: Technical report},
        author={Chollet, Francois and Knoop, Mike and Kamradt, Gregory and Landers, Bryan},
        journal={arXiv preprint arXiv:2412.04604},
        year={2024}
      }
      
      @misc{phan2025humanitysexamshort,
            title={Humanity's Last Exam}, 
            author={Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and others},
            year={2025},
            eprint={2501.14249},
            archivePrefix={arXiv},
            primaryClass={cs.LG},
            url={https://arxiv.org/abs/2501.14249}, 
      }
      
      
      @misc{phan2025humanitysexam,
            title={Humanity's Last Exam}, 
            author={Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and John Ling and Sean Shi and Michael Choi and Anish Agrawal and Arnav Chopra and Adam Khoja and Ryan Kim and Richard Ren and Jason Hausenloy and Oliver Zhang and Mantas Mazeika and Tung Nguyen and Daron Anderson and Imad Ali Shah and Mikhail Doroshenko and Alun Cennyth Stokes and Mobeen Mahmood and Jaeho Lee and Oleksandr Pokutnyi and Oleg Iskra and Jessica P. Wang and Robert Gerbicz and John-Clark Levin and Serguei Popov and Fiona Feng and Steven Y. Feng and Haoran Zhao and Michael Yu and Varun Gangal and Chelsea Zou and Zihan Wang and Mstyslav Kazakov and Geoff Galgon and Johannes Schmitt and Alvaro Sanchez and Yongki Lee and Will Yeadon and Scott Sauers and Marc Roth and Chidozie Agu and Søren Riis and Fabian Giska and Saiteja Utpala and Antrell Cheatom and Zachary Giboney and Gashaw M. Goshu and Sarah-Jane Crowson and Mohinder Maheshbhai Naiya and Noah Burns and Lennart Finke and Zerui Cheng and Hyunwoo Park and Francesco Fournier-Facio and Jennifer Zampese and John Wydallis and John B. Wydallis and Ryan G. Hoerr and Mark Nandor and Tim Gehrunger and Jiaqi Cai and Ben McCarty and Jungbae Nam and Edwin Taylor and Jun Jin and Gautier Abou Loume and Hangrui Cao and Alexis C Garretson and Damien Sileo and Qiuyu Ren and Doru Cojoc and Pavel Arkhipov and Usman Qazi and Aras Bacho and Lianghui Li and Sumeet Motwani and Christian Schroeder de Witt and Alexei Kopylov and Johannes Veith and Eric Singer and Paolo Rissone and Jaehyeok Jin and Jack Wei Lun Shi and Chris G. Willcocks and Ameya Prabhu and Longke Tang and Kevin Zhou and Emily de Oliveira Santos and Andrey Pupasov Maksimov and Edward Vendrow and Kengo Zenitani and Joshua Robinson and Aleksandar Mikov and Julien Guillod and Yuqi Li and Ben Pageler and Joshua Vendrow and Vladyslav Kuchkin and Pierre Marion and Denis Efremov and Jayson Lynch and Kaiqu Liang and Andrew Gritsevskiy and Dakotah Martinez and Nick Crispino and Dimitri Zvonkine and Natanael Wildner Fraga and Saeed Soori and Ori Press and Henry Tang and Julian Salazar and Sean R. Green and Lina Brüssel and Moon Twayana and Aymeric Dieuleveut and T. Ryan Rogers and Wenjin Zhang and Ross Finocchio and Bikun Li and Jinzhou Yang and Arun Rao and Gabriel Loiseau and Mikhail Kalinin and Marco Lukas and Ciprian Manolescu and Nate Stambaugh and Subrata Mishra and Ariel Ghislain Kemogne Kamdoum and Tad Hogg and Alvin Jin and Carlo Bosio and Gongbo Sun and Brian P Coppola and Haline Heidinger and Rafael Sayous and Stefan Ivanov and Joseph M Cavanagh and Jiawei Shen and Joseph Marvin Imperial and Philippe Schwaller and Shaipranesh Senthilkuma and Andres M Bran and Andres Algaba and Brecht Verbeken and Kelsey Van den Houte and Lynn Van Der Sypt and David Noever and Lisa Schut and Ilia Sucholutsky and Evgenii Zheltonozhskii and Qiaochu Yuan and Derek Lim and Richard Stanley and Shankar Sivarajan and Tong Yang and John Maar and Julian Wykowski and Martí Oller and Jennifer Sandlin and Anmol Sahu and Cesare Giulio Ardito and Yuzheng Hu and Felipe Meneguitti Dias and Tobias Kreiman and Kaivalya Rawal and Tobias Garcia Vilchis and Yuexuan Zu and Martin Lackner and James Koppel and Jeremy Nguyen and Daniil S. Antonenko and Steffi Chern and Bingchen Zhao and Pierrot Arsene and Sergey Ivanov and Rafał Poświata and Chenguang Wang and Daofeng Li and Donato Crisostomi and Ali Dehghan and Andrea Achilleos and John Arnold Ambay and Benjamin Myklebust and Archan Sen and David Perrella and Nurdin Kaparov and Mark H Inlow and Allen Zang and Kalyan Ramakrishnan and Daniil Orel and Vladislav Poritski and Shalev Ben-David and Zachary Berger and Parker Whitfill and Michael Foster and Daniel Munro and Linh Ho and Dan Bar Hava and Aleksey Kuchkin and Robert Lauff and David Holmes and Frank Sommerhage and Anji Zhang and Richard Moat and Keith Schneider and Daniel Pyda and Zakayo Kazibwe and Mukhwinder Singh and Don Clarke and Dae Hyun Kim and Sara Fish and Veit Elser and Victor Efren Guadarrama Vilchis and Immo Klose and Christoph Demian and Ujjwala Anantheswaran and Adam Zweiger and Guglielmo Albani and Jeffery Li and Nicolas Daans and Maksim Radionov and Václav Rozhoň and Vincent Ginis and Ziqiao Ma and Christian Stump and Jacob Platnick and Volodymyr Nevirkovets and Luke Basler and Marco Piccardo and Niv Cohen and Virendra Singh and Josef Tkadlec and Paul Rosu and Alan Goldfarb and Piotr Padlewski and Stanislaw Barzowski and Kyle Montgomery and Aline Menezes and Arkil Patel and Zixuan Wang and Jamie Tucker-Foltz and Jack Stade and Declan Grabb and Tom Goertzen and Fereshteh Kazemi and Jeremiah Milbauer and Abhishek Shukla and Hossam Elgnainy and Yan Carlos Leyva Labrador and Hao He and Ling Zhang and Alan Givré and Hew Wolff and Gözdenur Demir and Muhammad Fayez Aziz and Younesse Kaddar and Ivar Ängquist and Yanxu Chen and Elliott Thornley and Robin Zhang and Jiayi Pan and Antonio Terpin and Niklas Muennighoff and Hailey Schoelkopf and Eric Zheng and Avishy Carmi and Jainam Shah and Ethan D. L. Brown and Kelin Zhu and Max Bartolo and Richard Wheeler and Andrew Ho and Shaul Barkan and Jiaqi Wang and Martin Stehberger and Egor Kretov and Peter Bradshaw and JP Heimonen and Kaustubh Sridhar and Zaki Hossain and Ido Akov and Yury Makarychev and Joanna Tam and Hieu Hoang and David M. Cunningham and Vladimir Goryachev and Demosthenes Patramanis and Michael Krause and Andrew Redenti and David Aldous and Jesyin Lai and Shannon Coleman and Jiangnan Xu and Sangwon Lee and Ilias Magoulas and Sandy Zhao and Ning Tang and Michael K. Cohen and Micah Carroll and Orr Paradise and Jan Hendrik Kirchner and Stefan Steinerberger and Maksym Ovchynnikov and Jason O. Matos and Adithya Shenoy and Michael Wang and Yuzhou Nie and Paolo Giordano and Philipp Petersen and Anna Sztyber-Betley and Paolo Faraboschi and Robin Riblet and Jonathan Crozier and Shiv Halasyamani and Antonella Pinto and Shreyas Verma and Prashant Joshi and Eli Meril and Zheng-Xin Yong and Allison Tee and Jérémy Andréoletti and Orion Weller and Raghav Singhal and Gang Zhang and Alexander Ivanov and Seri Khoury and Nils Gustafsson and Hamid Mostaghimi and Kunvar Thaman and Qijia Chen and Tran Quoc Khánh and Jacob Loader and Stefano Cavalleri and Hannah Szlyk and Zachary Brown and Himanshu Narayan and Jonathan Roberts and William Alley and Kunyang Sun and Ryan Stendall and Max Lamparth and Anka Reuel and Ting Wang and Hanmeng Xu and Pablo Hernández-Cámara and Freddie Martin and Thomas Preu and Tomek Korbak and Marcus Abramovitch and Dominic Williamson and Ida Bosio and Ziye Chen and Biró Bálint and Eve J. Y. Lo and Maria Inês S. Nunes and Yibo Jiang and M Saiful Bari and Peyman Kassani and Zihao Wang and Behzad Ansarinejad and Yewen Sun and Stephane Durand and Guillaume Douville and Daniel Tordera and George Balabanian and Earth Anderson and Lynna Kvistad and Alejandro José Moyano and Hsiaoyun Milliron and Ahmad Sakor and Murat Eron and Isaac C. McAlister and Andrew Favre D. O. and Shailesh Shah and Xiaoxiang Zhou and Firuz Kamalov and Ronald Clark and Sherwin Abdoli and Tim Santens and Harrison K Wang and Evan Chen and Alessandro Tomasiello and G. Bruno De Luca and Shi-Zhuo Looi and Vinh-Kha Le and Noam Kolt and Niels Mündler and Avi Semler and Emma Rodman and Jacob Drori and Carl J Fossum and Luk Gloor and Milind Jagota and Ronak Pradeep and Honglu Fan and Tej Shah and Jonathan Eicher and Michael Chen and Kushal Thaman and William Merrill and Moritz Firsching and Carter Harris and Stefan Ciobâcă and Jason Gross and Rohan Pandey and Ilya Gusev and Adam Jones and Shashank Agnihotri and Pavel Zhelnov and Siranut Usawasutsakorn and Mohammadreza Mofayezi and Alexander Piperski and Marc Carauleanu and David K. Zhang and Kostiantyn Dobarskyi and Dylan Ler and Roman Leventov and Ignat Soroko and Thorben Jansen and Scott Creighton and Pascal Lauer and Joshua Duersch and Vage Taamazyan and Dario Bezzi and Wiktor Morak and Wenjie Ma and William Held and Tran Đuc Huy and Ruicheng Xian and Armel Randy Zebaze and Mohanad Mohamed and Julian Noah Leser and Michelle X Yuan and Laila Yacar and Johannes Lengler and Katarzyna Olszewska and Hossein Shahrtash and Edson Oliveira and Joseph W. Jackson and Daniel Espinosa Gonzalez and Andy Zou and Muthu Chidambaram and Timothy Manik and Hector Haffenden and Dashiell Stander and Ali Dasouqi and Alexander Shen and Emilien Duc and Bita Golshani and David Stap and Mikalai Uzhou and Alina Borisovna Zhidkovskaya and Lukas Lewark and Miguel Orbegozo Rodriguez and Mátyás Vincze and Dustin Wehr and Colin Tang and Shaun Phillips and Fortuna Samuele and Jiang Muzhen and Fredrik Ekström and Angela Hammon and Oam Patel and Faraz Farhidi and George Medley and Forough Mohammadzadeh and Madellene Peñaflor and Haile Kassahun and Alena Friedrich and Claire Sparrow and Rayner Hernandez Perez and Taom Sakal and Omkar Dhamane and Ali Khajegili Mirabadi and Eric Hallman and Kenchi Okutsu and Mike Battaglia and Mohammad Maghsoudimehrabani and Alon Amit and Dave Hulbert and Roberto Pereira and Simon Weber and Handoko and Anton Peristyy and Stephen Malina and Samuel Albanie and Will Cai and Mustafa Mehkary and Rami Aly and Frank Reidegeld and Anna-Katharina Dick and Cary Friday and Jasdeep Sidhu and Hassan Shapourian and Wanyoung Kim and Mariana Costa and Hubeyb Gurdogan and Brian Weber and Harsh Kumar and Tong Jiang and Arunim Agarwal and Chiara Ceconello and Warren S. Vaz and Chao Zhuang and Haon Park and Andrew R. Tawfeek and Daattavya Aggarwal and Michael Kirchhof and Linjie Dai and Evan Kim and Johan Ferret and Yuzhou Wang and Minghao Yan and Krzysztof Burdzy and Lixin Zhang and Antonio Franca and Diana T. Pham and Kang Yong Loh and Joshua Robinson and Abram Jackson and Shreen Gul and Gunjan Chhablani and Zhehang Du and Adrian Cosma and Jesus Colino and Colin White and Jacob Votava and Vladimir Vinnikov and Ethan Delaney and Petr Spelda and Vit Stritecky and Syed M. Shahid and Jean-Christophe Mourrat and Lavr Vetoshkin and Koen Sponselee and Renas Bacho and Florencia de la Rosa and Xiuyu Li and Guillaume Malod and Leon Lang and Julien Laurendeau and Dmitry Kazakov and Fatimah Adesanya and Julien Portier and Lawrence Hollom and Victor Souza and Yuchen Anna Zhou and Julien Degorre and Yiğit Yalın and Gbenga Daniel Obikoya and Luca Arnaboldi and Rai and Filippo Bigi and M. C. Boscá and Oleg Shumar and Kaniuar Bacho and Pierre Clavier and Gabriel Recchia and Mara Popescu and Nikita Shulga and Ngefor Mildred Tanwie and Denis Peskoff and Thomas C. H. Lux and Ben Rank and Colin Ni and Matthew Brooks and Alesia Yakimchyk and Huanxu and Liu and Olle Häggström and Emil Verkama and Hans Gundlach and Leonor Brito-Santana and Brian Amaro and Vivek Vajipey and Rynaa Grover and Yiyang Fan and Gabriel Poesia Reis e Silva and Linwei Xin and Yosi Kratish and Jakub Łucki and Wen-Ding Li and Sivakanth Gopi and Andrea Caciolai and Justin Xu and Kevin Joseph Scaria and Freddie Vargus and Farzad Habibi and Long and Lian and Emanuele Rodolà and Jules Robins and Vincent Cheng and Tony Fruhauff and Brad Raynor and Hao Qi and Xi Jiang and Ben Segev and Jingxuan Fan and Sarah Martinson and Erik Y. Wang and Kaylie Hausknecht and Michael P. Brenner and Mao Mao and Xinyu Zhang and David Avagian and Eshawn Jessica Scipio and Alon Ragoler and Justin Tan and Blake Sims and Rebeka Plecnik and Aaron Kirtland and Omer Faruk Bodur and D. P. Shinde and Zahra Adoul and Mohamed Zekry and Ali Karakoc and Tania C. B. Santos and Samir Shamseldeen and Loukmane Karim and Anna Liakhovitskaia and Nate Resman and Nicholas Farina and Juan Carlos Gonzalez and Gabe Maayan and Sarah Hoback and Rodrigo De Oliveira Pena and Glen Sherman and Elizabeth Kelley and Hodjat Mariji and Rasoul Pouriamanesh and Wentao Wu and Sandra Mendoza and Ismail Alarab and Joshua Cole and Danyelle Ferreira and Bryan Johnson and Mohammad Safdari and Liangti Dai and Siriphan Arthornthurasuk and Alexey Pronin and Jing Fan and Angel Ramirez-Trinidad and Ashley Cartwright and Daphiny Pottmaier and Omid Taheri and David Outevsky and Stanley Stepanic and Samuel Perry and Luke Askew and Raúl Adrián Huerta Rodríguez and Ali M. R. Minissi and Sam Ali and Ricardo Lorena and Krishnamurthy Iyer and Arshad Anil Fasiludeen and Sk Md Salauddin and Murat Islam and Juan Gonzalez and Josh Ducey and Maja Somrak and Vasilios Mavroudis and Eric Vergo and Juehang Qin and Benjámin Borbás and Eric Chu and Jack Lindsey and Anil Radhakrishnan and Antoine Jallon and I. M. J. McInnis and Pawan Kumar and Laxman Prasad Goswami and Daniel Bugas and Nasser Heydari and Ferenc Jeanplong and Archimedes Apronti and Abdallah Galal and Ng Ze-An and Ankit Singh and Joan of Arc Xavier and Kanu Priya Agarwal and Mohammed Berkani and Benedito Alves de Oliveira Junior and Dmitry Malishev and Nicolas Remy and Taylor D. Hartman and Tim Tarver and Stephen Mensah and Javier Gimenez and Roselynn Grace Montecillo and Russell Campbell and Asankhaya Sharma and Khalida Meer and Xavier Alapont and Deepakkumar Patil and Rajat Maheshwari and Abdelkader Dendane and Priti Shukla and Sergei Bogdanov and Sören Möller and Muhammad Rehan Siddiqi and Prajvi Saxena and Himanshu Gupta and Innocent Enyekwe and Ragavendran P V and Zienab EL-Wasif and Aleksandr Maksapetyan and Vivien Rossbach and Chris Harjadi and Mohsen Bahaloohoreh and Song Bian and John Lai and Justine Leon Uro and Greg Bateman and Mohamed Sayed and Ahmed Menshawy and Darling Duclosel and Yashaswini Jain and Ashley Aaron and Murat Tiryakioglu and Sheeshram Siddh and Keith Krenek and Alex Hoover and Joseph McGowan and Tejal Patwardhan and Summer Yue and Alexandr Wang and Dan Hendrycks},
            year={2025},
            eprint={2501.14249},
            archivePrefix={arXiv},
            primaryClass={cs.LG},
            url={https://arxiv.org/abs/2501.14249}, 
      }
      
      
      
      @article{yang2023looped,
        title={Looped transformers are better at learning learning algorithms},
        author={Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris},
        journal={arXiv preprint arXiv:2311.12424},
        year={2023}
      }
      
      @article{lake2017building,
        title={Building machines that learn and think like people},
        author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
        journal={Behavioral and brain sciences},
        volume={40},
        pages={e253},
        year={2017},
        publisher={Cambridge University Press}
      }
      
      @article{ren2024brain,
        title={Brain-inspired Artificial Intelligence: A Comprehensive Review},
        author={Ren, Jing and Xia, Feng},
        journal={arXiv preprint arXiv:2408.14811},
        year={2024}
      }
      
      
      @article{muller2018cortical,
        title={Cortical travelling waves: mechanisms and computational principles},
        author={Muller, Lyle and Chavane, Fr{\'e}d{\'e}ric and Reynolds, John and Sejnowski, Terrence J},
        journal={Nature Reviews Neuroscience},
        volume={19},
        number={5},
        pages={255--268},
        year={2018},
        publisher={Nature Publishing Group}
      }
      
      @article{jacobs2025traveling,
        title={Traveling Waves Integrate Spatial Information Into Spectral Representations},
        author={Jacobs, Mozes and Budzinski, Roberto C and Muller, Lyle and Ba, Demba and Keller, T Anderson},
        journal={arXiv preprint arXiv:2502.06034},
        year={2025}
      }
      
      @inproceedings{dauphin2017language,
        title={Language modeling with gated convolutional networks},
        author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
        booktitle={International conference on machine learning},
        pages={933--941},
        year={2017},
        organization={PMLR}
      }
      
      
      @misc{pytorch_torchvision_primitives_2021,
        author       = {Vryniotis, Vasilis and Cord, Matthieu},
        title        = {{How to Train State-Of-The-Art Models Using TorchVision's Latest Primitives}},
        howpublished = {PyTorch Blog},
        date         = {2021-12-22},
        year         = {2021},
        month        = {dec},
        url          = {https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/},
        urldate      = {2025-04-18},
        note         = {Last edited on 2021-12-22}
      }
      
      
      
      
      
      
      @inproceedings{xu2015show,
        title={Show, attend and tell: Neural image caption generation with visual attention},
        author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
        booktitle={International conference on machine learning},
        pages={2048--2057},
        year={2015},
        organization={PMLR}
      }
      @article{zhang2025t,
        title={T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model},
        author={Zhang, Tao and Pan, Jia-Shu and Feng, Ruiqi and Wu, Tailin},
        journal={arXiv preprint arXiv:2502.01989},
        year={2025}
      }
      @article{bansal2022end,
        title={End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking},
        author={Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
        journal={Advances in Neural Information Processing Systems},
        volume={35},
        pages={20232--20242},
        year={2022}
      }
      
</script>

<script language="javascript" type="text/javascript" src="lib/blazy.js"></script>
<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/controller.js"></script>